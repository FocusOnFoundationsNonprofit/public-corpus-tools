## metadata
last updated: 03-13-2024 Created
link: https://youtu.be/bOnBPGkxUuw
youtube title: AI Has System Breaking FUNDAMENTAL FLAW | Lex Fridman Podcast
youtube transcript source: auto-captions
length: 0:06:51

## content

### description (youtube)

A clip from Lex Fridman podcast available on YouTube.
Episode #416 Yann LeCun
Yann LeCun is the Chief AI Scientist at Meta, professor at NYU, Turing Award winner, and one of the most influential researchers in the history of AI.

### transcript (youtube)

wonder if you could talk about hallucinations from your perspectives the why hallucinations happen from large language models and why and to what degree is that a fundamental flaw of large language models right so because of the auto regressive prediction every time an LM produces a token or word uh there is some level of probability for that word to take you out of the set of reasonable answers uh and if you assume which is a very strong assumption that the probability of such error um is that those errors are independent across a sequence of tokens being produced what that means is that every time you produce a token the probability that you rest you you stay within the the set of correct answer decreases and it decreases exponentially so there's a strong like you said assumption there that if uh there's a nonzero probability of making a mistake which there appears to be then there's going to be a kind of drift yeah and that drift is exponential it's like errors accumulate right so so the probability that an answer would be nonsensical increases exponentially with the number of tokens is that obvious to you by the way like well so mathematically speaking maybe but like isn't there a kind of gravitational pull towards the truth because on on average hopefully the truth is well represented in the uh training set no it's basically a struggle against uh the curse of dimensionality so the way you can correct for this is that you fine-tune the system by having it produce answers for all kinds of questions that people might come up with MH and people are people so they a lot of the questions that they have are very similar to each other so you can probably cover you know 80% or whatever of questions that people will will ask um by you know collecting data and then um and then you find you the system to produce good answers for all of those things and it's probably going to be able to learn that because it's got a lot of capacity to to learn U but then there is you know the enormous set of prompts that you have not covered during training and that set is enormous like within the set of all possible prompts the proportion prompts that have been uh used for training is absolutely tiny um it's it's a tiny tiny tiny subset of all possible prompts and so the system will behave properly on the prompts that has been either trained pre-trained or fine-tuned um but then there is an entire space of things that it cannot possibly have been trained on because it's just the number is gigantic so um so whatever training the system uh has been subject to to produce appropriate tensors you can break it by finding out a prompt that will be outside of the the the set of promps has been trained on or things that are similar and then it will just spew complete nonsense do when you say prompt do you mean that exact prompt or do you mean a prompt that's like in many parts very different than like is it that easy to ask a question or to say a thing that has been said before on the internet I mean people have come up with things where like you you put a essentially a random sequence of characters in The Prompt and that's enough to kind of throw the system uh into a mode where you know it it's going to answer something completely different than it would have answered without this so that's a way to jailbreak the system basically get it you know go outside of its uh of its conditioning right so that that's a very clear demonstration of it but of course uh uh you know that's uh that goes outside of what is designed to do right if you actually stitch together reasonably grammatical sentences is that is it that easy to break it yeah some people have done things like you you you write a sentence in English right that has and or you ask a question in English and it it produces a perfectly fine answer and then you just substitute a few words by the same word in another language and all of a sudden the answer is complet yeah so so I guess what I'm saying is like which fraction of prompts that humans are likely to generate are going to break the system so the the problem is that there is a long tail yes uh this is a an issue that a lot of people have realized you know in social networks and stuff like that which is uh there's a very very long taale of of things that people will ask and you can fine-tune the system for the 80% or whatever of uh the things that most people will will ask and then this long tail is is so large that you're not going to be able to function the system for all the conditions and in the end the system ends up being kind of a giant lookup table right essentially which is not really what you want you want system second reason certainly they can plan so the type of reasoning that takes place in llm is very very primitive and the reason you can tell is primitive is because the amount of computation that is spent per token produced is constant so if you ask a question and that question has an answer in a given number of token the amount of competition devoted to Computing that answer can be exactly estimated it's like you know it's how it's the the size of the prediction Network you know with its 36 layers or 92 layers or whatever it is uh multiply by number of tokens that's it and so essentially it doesn't matter if the question being asked is is simp simple to answer complicated to answer impossible to answer because it's undecidable or something um the amount of computation the system will be able to devote to that to the answer is constant or is number of token produced in the answer right this is not the way we work the way we reason is that when we're faced with a complex problem or complex question we spend more time trying to solve it and answer it right because it's more difficult there's a prediction element there's a iterative element where you're like uh adjusting your understanding of a thing by going over and over and over there's a hierarchical element so on does this mean it's a fundamental flaw of llms or does it mean that there's more part to that question
