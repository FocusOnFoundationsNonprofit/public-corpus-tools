{
  "nodes": [
    {
      "id": "fileops.custom_formatwarning",
      "label": "custom_formatwarning",
      "group": "function",
      "module": "fileops.py",
      "submodule": "INITIAL",
      "def": "def custom_formatwarning(msg, category, filename, lineno, line=None):",
      "line": 20,
      "docstring": "DO NOT CALL - only used to define the custom format"
    },
    {
      "id": "fileops.verbose_print",
      "label": "verbose_print",
      "group": "function",
      "module": "fileops.py",
      "submodule": "INITIAL",
      "def": "def verbose_print(verbose, *messages):",
      "line": 28,
      "docstring": "Helper function to pass on bool verbose and make verbose printing cleaner\n    \n    :param verbose: boolean for whether to print the messages.\n    :param messages: tuple of variable-length argument list.\n    :return: None"
    },
    {
      "id": "fileops.check_file_exists",
      "label": "check_file_exists",
      "group": "function",
      "module": "fileops.py",
      "submodule": "INITIAL",
      "def": "def check_file_exists(file_path, operation_name):",
      "line": 42,
      "docstring": "Checks file existence and raises a ValueError if not found, which stops execution.\n\n    :param file_path: string of file path which can be absolute or relative.\n    :param operation_name: string of the message that the ValueError will print, typically the function name - optional message.\n    :return: bool, True if file exists, False otherwise."
    },
    {
      "id": "fileops.warn_file_overwrite",
      "label": "warn_file_overwrite",
      "group": "function",
      "module": "fileops.py",
      "submodule": "INITIAL",
      "def": "def warn_file_overwrite(file_path):",
      "line": 53,
      "docstring": "Checks if a file already exists and issues a warning if it does.\n\n    :param file_path: string representing the path of the file to be checked.\n    :return: bool, True if file exists, False otherwise."
    },
    {
      "id": "fileops.get_suffix",
      "label": "get_suffix",
      "group": "function",
      "module": "fileops.py",
      "submodule": "SUFFIX",
      "def": "def get_suffix(file_str, delimiter='_'):",
      "line": 68,
      "docstring": "Extracts the suffix from a given file string based on a specified delimiter.\n\n    :param file_str: string of the file name from which to extract the suffix.\n    :param delimiter: string representing the delimiter used to separate the suffix from the rest of the file string. Default is '_'.\n    :return: string of the extracted suffix or None if no valid suffix is found."
    },
    {
      "id": "fileops.add_suffix_in_str",
      "label": "add_suffix_in_str",
      "group": "function",
      "module": "fileops.py",
      "submodule": "SUFFIX",
      "def": "def add_suffix_in_str(file_str, suffix_add):",
      "line": 110,
      "docstring": "Adds a suffix to a given file string.\n\n    :param file_str: string of the file name to which the suffix will be added.\n    :param suffix_add: string of the suffix to be added to the file string.\n    :return: string of the file name with the added suffix."
    },
    {
      "id": "fileops.sub_suffix_in_str",
      "label": "sub_suffix_in_str",
      "group": "function",
      "module": "fileops.py",
      "submodule": "SUFFIX",
      "def": "def sub_suffix_in_str(file_str, suffix_sub, delimiter='_'):",
      "line": 127,
      "docstring": "Replaces the existing suffix in a file string with a new suffix.\n\n    :param file_str: string of the file name from which to replace the suffix.\n    :param suffix_sub: string of the new suffix to replace the existing one.\n    :param delimiter: string representing the delimiter used to separate the suffix from the rest of the file string. Default is '_'.\n    :return: string of the file name with the replaced suffix or the original file string if no valid suffix is found."
    },
    {
      "id": "fileops.remove_all_suffixes_in_str",
      "label": "remove_all_suffixes_in_str",
      "group": "function",
      "module": "fileops.py",
      "submodule": "SUFFIX",
      "def": "def remove_all_suffixes_in_str(file_str, delimiter='_'): # DS, cat 1, unitests 7 - no mock",
      "line": 153,
      "docstring": "Removes all suffixes from a given file string based on a specified delimiter while retaining the original file extension.\n\n    :param file_str: string of the file name from which to remove all suffixes.\n    :param delimiter: string representing the delimiter used to separate the suffixes from the rest of the file string. Default is '_'.\n    :return: string of the file name with all suffixes removed but with the original extension preserved."
    },
    {
      "id": "fileops.copy_file_and_append_suffix",
      "label": "copy_file_and_append_suffix",
      "group": "function",
      "module": "fileops.py",
      "submodule": "SUFFIX",
      "def": "def copy_file_and_append_suffix(file_path, suffix_new):",
      "line": 185,
      "docstring": "Copies the file with a new suffix added before the file extension.\n\n    :param file_path: string of the path to the original file.\n    :param suffix_new: string of the suffix to be appended to the original filename before the file extension.\n    :return: string of the path to the newly created file with the new suffix."
    },
    {
      "id": "fileops.sub_suffix_in_file",
      "label": "sub_suffix_in_file",
      "group": "function",
      "module": "fileops.py",
      "submodule": "SUFFIX",
      "def": "def sub_suffix_in_file(file_path, suffix_new):",
      "line": 208,
      "docstring": "Substitutes the suffix in the file name of the given file path with a new suffix.\n    If new_suffix is empty '' then it will remove the last suffix of the file.\n\n    :param file_path: string, the path to the original file.\n    :param suffix_new: string, the new suffix to replace the existing one in the file name.\n    :return: string, the path to the newly created file with the substituted suffix."
    },
    {
      "id": "fileops.count_suffixes_in_folder",
      "label": "count_suffixes_in_folder",
      "group": "function",
      "module": "fileops.py",
      "submodule": "SUFFIX",
      "def": "def count_suffixes_in_folder(folder_path):",
      "line": 233,
      "docstring": "Analyzes all the files in the specified folder and prints the number of files for each unique suffix, alphabetized.\n\n    :param folder_path: string of the folder path to search for files and analyze suffixes.\n    :return: None. The function prints the suffixes and their counts."
    },
    {
      "id": "fileops.get_files_in_folder",
      "label": "get_files_in_folder",
      "group": "function",
      "module": "fileops.py",
      "submodule": "FOLDER",
      "def": "def get_files_in_folder(folder_path, suffixpat_include=None, suffixpat_exclude=None, include_subfolders=False):",
      "line": 256,
      "docstring": "Retrieves a list of file paths from the specified folder, sorted in alphabetical order.\n    Optionally filters by suffix pattern and includes subfolders.\n\n    :param folder_path: string of the path to the folder from which to retrieve files.\n    :param suffixpat_include: string of the suffix pattern that included files must have.\n    :param suffixpat_exclude: string of the suffix pattern that files must not have to be included.\n    :param include_subfolders: boolean indicating whether to include files from subfolders.\n    :return: list of strings of the file paths that meet the specified criteria."
    },
    {
      "id": "fileops.apply_to_folder",
      "label": "apply_to_folder",
      "group": "function",
      "module": "fileops.py",
      "submodule": "FOLDER",
      "def": "def apply_to_folder(worker_function, folder_path, *args, suffixpat_include=None, suffixpat_exclude=None, include_subfolders=False, verbose=False, **kwargs):",
      "line": 304,
      "docstring": "Controller function that applies a specified worker function to each file in a folder.\n    The worker function must operate on a single file.\n    If it needs the folder name, that can be extracted from the file path.\n    If it's creating or writing to another file, that filename or path can be given as another argument.\n\n    :param worker_function: worker function name to apply to each file, not a string so do not use quotes.\n    :param folder_path: string of the path to the folder from which to retrieve files.\n    :param args: additional arguments to pass to the function.\n    :param suffixpat_include: string of the suffix pattern that included files must have.\n    :param suffixpat_exclude: string of the suffix pattern that files must not have to be included.\n    :param include_subfolders: boolean of whether to include files from subfolders.\n    :param verbose: boolean for printing verbose messages. Defaults to True.\n    :param kwargs: additional keyword arguments to pass to the function.\n    :return: a dictionary with file paths as keys and function return values as values."
    },
    {
      "id": "fileops.read_complete_text",
      "label": "read_complete_text",
      "group": "function",
      "module": "fileops.py",
      "submodule": "READ WRITE",
      "def": "def read_complete_text(file_path):",
      "line": 339,
      "docstring": "Reads the entire text from a file.\n\n    :param file_path: string of the file path which can be absolute or relative.\n    :return: string of the complete text read from the file."
    },
    {
      "id": "fileops.read_metadata_and_content",
      "label": "read_metadata_and_content",
      "group": "function",
      "module": "fileops.py",
      "submodule": "READ WRITE",
      "def": "def read_metadata_and_content(file_path):",
      "line": 352,
      "docstring": "Reads the text from a file and splits it into the metadata and content sections.\n    Gives a ValueError if both metadata and content are not present in one of the 2 formats.\n    Format 1: ## metadata and ## content\n    Format 2: METADATA and CONTENT\n    Strips all leading and trailing newlines from the metadata string.\n    The content string starts with the content delimiter.\n\n    :param file_path: string of the file path which can be absolute or relative.\n    :return: tuple, the metadata and content as two separate strings."
    },
    {
      "id": "fileops.read_file_flex",
      "label": "read_file_flex",
      "group": "function",
      "module": "fileops.py",
      "submodule": "READ WRITE",
      "def": "def read_file_flex(file_path):",
      "line": 388,
      "docstring": "Reads the text from a file and splits it into the metadata and content sections if present.\n    If no metadata is found, returns (None, complete_text).\n\n    :param file_path: string of the file path which can be absolute or relative.\n    :return: tuple, the metadata and content as two separate strings. If no metadata, returns (None, complete_text)."
    },
    {
      "id": "fileops.handle_overwrite_prompt",
      "label": "handle_overwrite_prompt",
      "group": "function",
      "module": "fileops.py",
      "submodule": "READ WRITE",
      "def": "def handle_overwrite_prompt(file_path, file_path_opfunc, verbose=True):",
      "line": 418,
      "docstring": "Handles user prompt for overwriting a file.\n\n    :param file_path: string of the original file path.\n    :param file_path_opfunc: string of the new file path.\n    :param verbose: boolean for whether to print verbose messages. Default is True.\n    :return: string of the path to the file that was kept."
    },
    {
      "id": "fileops.manage_file_overwrite",
      "label": "manage_file_overwrite",
      "group": "function",
      "module": "fileops.py",
      "submodule": "READ WRITE",
      "def": "def manage_file_overwrite(original_path, suffix_new, overwrite, verbose=False):",
      "line": 451,
      "docstring": "Handle file overwriting based on the specified mode.\n    \n    :param original_path: string, the path to the original file.\n    :param suffix_new: string, the suffix to be appended to the original filename for the new file.\n    :param overwrite: string, the overwrite mode ('no', 'no-sub', 'replace', 'replace-sub', 'yes', 'prompt').\n    :param verbose: boolean, whether to print verbose messages.\n    :return: string, the final path of the file after applying overwrite logic."
    },
    {
      "id": "fileops.write_complete_text",
      "label": "write_complete_text",
      "group": "function",
      "module": "fileops.py",
      "submodule": "READ WRITE",
      "def": "def write_complete_text(file_path, complete_text, suffix_new='_temp', overwrite='no', verbose=False):",
      "line": 495,
      "docstring": "Writes the complete text to a new file with a specified suffix and handles overwrite logic.\n\n    :param file_path: string, the path to the original file.\n    :param complete_text: string, the complete text to be written to the new file.\n    :param suffix_new: string, the suffix to be appended to the original filename for the new file.\n    :param overwrite: string, the overwrite mode. Default is 'no'.\n    :param verbose: boolean, whether to print verbose messages. Default is False.\n    :return: string, the path to the final file after applying overwrite logic."
    },
    {
      "id": "fileops.write_metadata_and_content",
      "label": "write_metadata_and_content",
      "group": "function",
      "module": "fileops.py",
      "submodule": "READ WRITE",
      "def": "def write_metadata_and_content(file_path, metadata, content, suffix_new='_temp', overwrite='no', verbose=False):",
      "line": 523,
      "docstring": "Writes the metadata and content text to a new file with a specified suffix and handles overwrite logic.\n    Insert 2 blank lines between the metadata and content sections if metadata is present or empty string.\n\n    :param file_path: string, the path to the original file.\n    :param metadata: string or None, the metadata section to be written to the new file, inclusive of '## metadata' or 'METADATA'.\n    :param content: string, the content section to be written to the new file, inclusive of '## content' or 'CONTENT'.\n    :param suffix_new: string, the suffix to be appended to the original filename for the new file.\n    :param overwrite: string, the overwrite mode. Default is 'no'.\n    :param verbose: boolean, whether to print verbose messages. Default is False.\n    :return: string, the path to the final file after applying overwrite logic."
    },
    {
      "id": "fileops.pretty_print_json_structure",
      "label": "pretty_print_json_structure",
      "group": "function",
      "module": "fileops.py",
      "submodule": "JSON",
      "def": "def pretty_print_json_structure(json_file_path, level_limit=None, save_to_file=False):",
      "line": 545,
      "docstring": "Prints the structure of a JSON file and optionally saves it to a file with a '.pretty' extension.\n\n    :param json_file_path: string of the path to the json file.\n    :param level_limit: integer of the maximum level of nesting to print. None means no limit.\n    :param save_to_file: boolean indicating whether to save the output to a file. defaults to true.\n    :return: None."
    },
    {
      "id": "fileops.print_json_structure",
      "label": "print_json_structure",
      "group": "function",
      "module": "fileops.py",
      "submodule": "JSON",
      "def": "def print_json_structure(data, indent=0, parent_key='', level=0):",
      "line": 562,
      "docstring": "Writes a JSON object to a file at the specified path.\n\n    :param json_object: dictionary or list to be written as JSON.\n    :param file_path: string of the path where the JSON file will be written.\n    :param overwrite: string of either 'yes' or 'no' to determine if existing files should be overwritten. default is 'no'.\n    :return: None."
    },
    {
      "id": "fileops.write_json_file_from_object",
      "label": "write_json_file_from_object",
      "group": "function",
      "module": "fileops.py",
      "submodule": "JSON",
      "def": "def write_json_file_from_object(json_object, file_path, overwrite=\"no\"):",
      "line": 599,
      "docstring": "Writes a JSON object to a file at the specified path.\n\n    :param json_object: dictionary or list to be written as JSON.\n    :param file_path: string of the path where the JSON file will be written.\n    :param overwrite: string of either 'yes' or 'no' to determine if existing files should be overwritten. default is 'no'.\n    :return: None."
    },
    {
      "id": "fileops.read_json_object_from_file",
      "label": "read_json_object_from_file",
      "group": "function",
      "module": "fileops.py",
      "submodule": "JSON",
      "def": "def read_json_object_from_file(file_path):  # consider moving to fileops",
      "line": 617,
      "docstring": "Reads a JSON object from a file at the specified path.\n\n    :param file_path: string of the path to the JSON file to be read.\n    :return: dictionary or list representing the JSON object read from the file."
    },
    {
      "id": "fileops.rename_file",
      "label": "rename_file",
      "group": "function",
      "module": "fileops.py",
      "submodule": "MISC FILE",
      "def": "def rename_file(file_path, new_filebase):",
      "line": 629,
      "docstring": "Renames the file base portion for the file given at the argument file path.\n\n    :param file_path: string, the path to the file to be renamed.\n    :param new_filebase: string, the new base name for the file without the extension.\n    :return: string, the new file path after renaming, or an error if the operation fails."
    },
    {
      "id": "fileops.rename_file_extension",
      "label": "rename_file_extension",
      "group": "function",
      "module": "fileops.py",
      "submodule": "MISC FILE",
      "def": "def rename_file_extension(file_path, new_extension):",
      "line": 654,
      "docstring": "Renames the file extension for the file given at the argument file path.\n\n    :param file_path: string, the path to the file to be renamed.\n    :param new_extension: string, the new extension for the file (including the dot).\n    :return: string, the new file path after renaming, or an error if the operation fails."
    },
    {
      "id": "fileops.delete_file",
      "label": "delete_file",
      "group": "function",
      "module": "fileops.py",
      "submodule": "MISC FILE",
      "def": "def delete_file(file_path):",
      "line": 679,
      "docstring": "Deletes a file at the specified file path.\n\n    :param file_path: string, the path to the file to be deleted.\n    :return: None. The function does not return any value."
    },
    {
      "id": "fileops.delete_files_with_suffix",
      "label": "delete_files_with_suffix",
      "group": "function",
      "module": "fileops.py",
      "submodule": "MISC FILE",
      "def": "def delete_files_with_suffix(folder, suffixpat_include, verbose=False):  # omit unittests",
      "line": 691,
      "docstring": "Deletes all files in a given folder that end with a specified suffix.\n\n    :param folder_path: string, the path to the folder where files are to be deleted.\n    :param suffixpat_include: string, the suffix pattern of the files to be deleted.\n    :param verbose: boolean, if True, the function will print verbose messages. Default is False.\n    :return: None. The function does not return any value."
    },
    {
      "id": "fileops.move_file",
      "label": "move_file",
      "group": "function",
      "module": "fileops.py",
      "submodule": "MISC FILE",
      "def": "def move_file(file_path, destination_folder): # cat 3a, unittest 3 - mocks",
      "line": 702,
      "docstring": "Moves a file to the specified destination folder.\n\n    :param file_path: string, the path to the file to be moved.\n    :param destination_folder: string, the path to the destination folder.\n    :return: string, the new file path after moving, or an error if the operation fails."
    },
    {
      "id": "fileops.move_files_with_suffix",
      "label": "move_files_with_suffix",
      "group": "function",
      "module": "fileops.py",
      "submodule": "MISC FILE",
      "def": "def move_files_with_suffix(source_folder, destination_folder, suffixpat_include, verbose=False): # omit unittest",
      "line": 728,
      "docstring": "Moves all files in a given folder that end with a specified suffix to the destination folder.\n\n    :param source_folder: string, the path to the source folder where files are to be moved from.\n    :param destination_folder: string, the path to the destination folder where files are to be moved to.\n    :param suffixpat_include: string, the suffix pattern of the files to be moved.\n    :param verbose: boolean, if True, the function will print verbose messages. Default is False.\n    :return: list, the new file paths after moving, or an error if the operation fails."
    },
    {
      "id": "fileops.tune_title",
      "label": "tune_title",
      "group": "function",
      "module": "fileops.py",
      "submodule": "MISC FILE",
      "def": "def tune_title(title):",
      "line": 740,
      "docstring": "Removes any special characters from the given title.\n\n    :param title: string, the title from which special characters are to be removed.\n    :return: string, the updated title with special characters removed."
    },
    {
      "id": "fileops.create_full_path",
      "label": "create_full_path",
      "group": "function",
      "module": "fileops.py",
      "submodule": "MISC FILE",
      "def": "def create_full_path(title_or_path, new_suffix_ext, default_folder=None):",
      "line": 749,
      "docstring": "Creates a full file path from a given title or path, a new suffix extension, and an optional default folder.\n\n    :param title_or_path: string, the title or path of the file.\n    :param new_suffix_ext: string, the new suffix extension to be added to the file.\n    :param default_folder: string, the default folder to be used if no folder is specified in title_or_path. Default is None.\n    :return: string, the newly created full file path."
    },
    {
      "id": "fileops.find_file_in_folders",
      "label": "find_file_in_folders",
      "group": "function",
      "module": "fileops.py",
      "submodule": "MISC FILE",
      "def": "def find_file_in_folders(file_path, folder_paths):",
      "line": 782,
      "docstring": "Searches for a file within a list of folder paths and returns the first match.\n\n    :param file_path: string of the file name to search for.\n    :param folder_paths: list of strings of folder paths where the file will be searched.\n    :return: string of the full path to the file if found, otherwise None."
    },
    {
      "id": "fileops.zip_files_in_folders",
      "label": "zip_files_in_folders",
      "group": "function",
      "module": "fileops.py",
      "submodule": "MISC FILE",
      "def": "def zip_files_in_folders(folder_paths, suffixpat_include, zip_file_path, include_subfolders=True):",
      "line": 799,
      "docstring": "Zips files in the specified folders that match the given suffix into a single zip file.\n\n    :param folder_paths: list of strings, the paths to the folders where files will be zipped.\n    :param suffixpat_include: string, the suffix pattern that included files must have.\n    :param zip_file_path: string, the path where the single zip file will be created.\n    :param include_subfolders: boolean, indicates whether to include files from subfolders.\n    :return: None"
    },
    {
      "id": "fileops.compare_files_text",
      "label": "compare_files_text",
      "group": "function",
      "module": "fileops.py",
      "submodule": "MISC FILE",
      "def": "def compare_files_text(file1_path, file2_path):",
      "line": 839,
      "docstring": "Compares the content of two files.\n\n    :param file1_path: string, the path to the first file to be compared.\n    :param file2_path: string, the path to the second file to be compared.\n    :return: boolean, True if the content of the files is exactly the same, False otherwise."
    },
    {
      "id": "fileops.get_text_between_delimiters",
      "label": "get_text_between_delimiters",
      "group": "function",
      "module": "fileops.py",
      "submodule": "MISC FILE",
      "def": "def get_text_between_delimiters(full_text, delimiter_start, delimiter_end=None):",
      "line": 855,
      "docstring": "Extracts a substring from the given text between specified start and end delimiters.\n\n    :param full_text: string of the text from which to extract the substring.\n    :param delimiter_start: string of the delimiter indicating the start of the substring.\n    :param delimiter_end: string of the delimiter indicating the end of the substring. If None, the end of the text is used. Default is None.\n    :return: string of the extracted substring (inclusive of delimiter_start), or None if the start delimiter is not found."
    },
    {
      "id": "fileops.check_if_duplicate_filename",
      "label": "check_if_duplicate_filename",
      "group": "function",
      "module": "fileops.py",
      "submodule": "MISC FILE",
      "def": "def check_if_duplicate_filename(filename, folder, exclude_suffix=True):",
      "line": 878,
      "docstring": "Checks if a filename already exists in a given folder, optionally excluding suffixes.\n\n    :param filename: str, the filename to check for duplicates.\n    :param folder: str, the path to the folder to search in.\n    :param exclude_suffix: bool, whether to exclude suffixes when comparing filenames.\n    :return: bool, True if a duplicate is found, False otherwise."
    },
    {
      "id": "fileops.convert_seconds_to_timestamp",
      "label": "convert_seconds_to_timestamp",
      "group": "function",
      "module": "fileops.py",
      "submodule": "TIME AND TIMESTAMP",
      "def": "def convert_seconds_to_timestamp(seconds):",
      "line": 911,
      "docstring": "Converts a given number of seconds into a timestamp in the format hh:mm:ss or mm:ss.\n\n    :param seconds: integer or float representing the number of seconds to be converted.\n    :return: string representing the timestamp in the format hh:mm:ss or mm:ss."
    },
    {
      "id": "fileops.convert_timestamp_to_seconds",
      "label": "convert_timestamp_to_seconds",
      "group": "function",
      "module": "fileops.py",
      "submodule": "TIME AND TIMESTAMP",
      "def": "def convert_timestamp_to_seconds(timestamp):",
      "line": 935,
      "docstring": "Converts a timestamp in the format hh:mm:ss or mm:ss into seconds.\n\n    :param timestamp: string representing the timestamp in the format hh:mm:ss or mm:ss.\n    :return: integer representing the total number of seconds."
    },
    {
      "id": "fileops.change_timestamp",
      "label": "change_timestamp",
      "group": "function",
      "module": "fileops.py",
      "submodule": "TIME AND TIMESTAMP",
      "def": "def change_timestamp(timestamp, delta_seconds):",
      "line": 969,
      "docstring": "Changes a given timestamp by a specified number of seconds. Uncomment line in tune_timestamp\n\n    :param timestamp: string representing the timestamp in the format hh:mm:ss or mm:ss.\n    :param delta_seconds: integer representing the number of seconds to change the timestamp by.\n    :return: string representing the new timestamp after the change."
    },
    {
      "id": "fileops.tune_timestamp",
      "label": "tune_timestamp",
      "group": "function",
      "module": "fileops.py",
      "submodule": "TIME AND TIMESTAMP",
      "def": "def tune_timestamp(timestamp):",
      "line": 983,
      "docstring": "Converts a given timestamp to a standard format with respect to digits and leading zeros.\n\n    :param timestamp: string representing the timestamp in the format hh:mm:ss or mm:ss.\n    :return: string representing the tuned timestamp or None if the input timestamp is None."
    },
    {
      "id": "fileops.get_timestamp",
      "label": "get_timestamp",
      "group": "function",
      "module": "fileops.py",
      "submodule": "TIME AND TIMESTAMP",
      "def": "def get_timestamp(line, print_line=False, max_words=8):",
      "line": 998,
      "docstring": "Extracts a timestamp from a given line of text.\n\n    :param line: string representing the line of text to search for a timestamp.\n    :param print_line: boolean indicating whether to print the line where the timestamp was found. Default is False.\n    :param max_words: integer representing the maximum number of words allowed before and after the timestamp. Default is 5.\n    :return: tuple containing the extracted timestamp as a string and its index in the line, or (None, None) if no valid timestamp is found."
    },
    {
      "id": "fileops.get_current_datetime_humanfriendly",
      "label": "get_current_datetime_humanfriendly",
      "group": "function",
      "module": "fileops.py",
      "submodule": "TIME AND TIMESTAMP",
      "def": "def get_current_datetime_humanfriendly(timezone='America/Los_Angeles', include_timezone=True):",
      "line": 1058,
      "docstring": "Returns the current date and time as a string for a given timezone, optionally including the timezone abbreviation and UTC offset.\n\n    :param timezone: string representing the timezone to use for the current time.\n    :param include_timezone: boolean indicating whether to include the timezone abbreviation and UTC offset in the returned string.\n    :return: string representing the current date and time in the specified timezone, optionally followed by the timezone abbreviation and UTC offset."
    },
    {
      "id": "fileops.get_current_datetime_filefriendly",
      "label": "get_current_datetime_filefriendly",
      "group": "function",
      "module": "fileops.py",
      "submodule": "TIME AND TIMESTAMP",
      "def": "def get_current_datetime_filefriendly(location='America/Los_Angeles', include_utc=False):",
      "line": 1075,
      "docstring": "Returns the current date and time as a filename-friendly string for a given timezone, optionally including only the UTC offset.\n\n    :param location: string representing the timezone to use for the current time.\n    :param include_utc: boolean indicating whether to include the UTC offset in the returned string.\n    :return: string representing the current date and time in the specified timezone, formatted for filenames, optionally followed by the UTC offset."
    },
    {
      "id": "fileops.convert_to_epoch_seconds",
      "label": "convert_to_epoch_seconds",
      "group": "function",
      "module": "fileops.py",
      "submodule": "TIME AND TIMESTAMP",
      "def": "def convert_to_epoch_seconds(datetime_flex, timezone='America/Los_Angeles', verbose=False):",
      "line": 1097,
      "docstring": "Converts a human-readable time to the number of seconds since the Unix epoch (1970-01-01 00:00:00 UTC).\n    The function assumes the input format is 'YYYY-MM-DD_HH:MM:SS [optional UTC offset] [optional timezone]', \n    where the date and time are separated by an underscore or space,\n    and the time may be followed by another underscore or space and a UTC offset string, and optionally a timezone string.\n\n    :param datetime_flex: string representing the time, which can be in 'YYYY-MM-DD_HH:MM:SS' format or 'YYYY-MM-DD HH:MM:SS [UTC offset] [timezone]'.\n    :param default_timezone: string representing the default timezone if not specified in datetime_flex. Defaults to 'America/Los_Angeles'.\n    :return: float representing the number of seconds since the Unix epoch."
    },
    {
      "id": "fileops.get_elapsed_seconds",
      "label": "get_elapsed_seconds",
      "group": "function",
      "module": "fileops.py",
      "submodule": "TIME AND TIMESTAMP",
      "def": "def get_elapsed_seconds(start_time_epoch_seconds):",
      "line": 1183,
      "docstring": "Calculates the time elapsed since the start_time and returns it in seconds.\n\n    :param start_time: float, the start time in seconds since the epoch (as returned by time.time())\n    :param return_format: string ('minutes' or 'timestamp') for the return value format - 'timestamp' for H:MM:SS or minutes with one decimal place\n    :return: integer of the number of seconds"
    },
    {
      "id": "fileops.remove_timestamp_links_from_content",
      "label": "remove_timestamp_links_from_content",
      "group": "function",
      "module": "fileops.py",
      "submodule": "TIMESTAMP LINKS",
      "def": "def remove_timestamp_links_from_content(content):",
      "line": 1195,
      "docstring": "Removes markdown timestamp links from the content and returns the modified content.\n\n    :param content: string of the content from which to remove markdown timestamp links.\n    :return: string of the content with markdown timestamp links removed."
    },
    {
      "id": "fileops.remove_timestamp_links",
      "label": "remove_timestamp_links",
      "group": "function",
      "module": "fileops.py",
      "submodule": "TIMESTAMP LINKS",
      "def": "def remove_timestamp_links(file_path):",
      "line": 1206,
      "docstring": "Removes markdown timestamp links and overwrites the file.\n\n    :param file_path: string of the path to the original file.\n    :return: none."
    },
    {
      "id": "fileops.generate_timestamp_link",
      "label": "generate_timestamp_link",
      "group": "function",
      "module": "fileops.py",
      "submodule": "TIMESTAMP LINKS",
      "def": "def generate_timestamp_link(base_link, timestamp):",
      "line": 1220,
      "docstring": "Helper function to generate a timestamp link for a given base link and timestamp.\n    It uses a dictionary to map domains to their respective timestamp formats.\n    For Vimeo, the timestamp is converted to milliseconds.\n    \n    :param base_link: string of the base URL to which the timestamp will be appended.\n    :param timestamp: string of the timestamp to be converted and appended to the base URL.\n    :return: string of the complete URL with the timestamp appended in the appropriate format."
    },
    {
      "id": "fileops.add_timestamp_links_to_content",
      "label": "add_timestamp_links_to_content",
      "group": "function",
      "module": "fileops.py",
      "submodule": "TIMESTAMP LINKS",
      "def": "def add_timestamp_links_to_content(content, base_link):",
      "line": 1257,
      "docstring": "Adds timestamp links to the content using the provided base link.\n\n    :param content: string of the content where timestamp links will be added.\n    :param base_link: string of the base URL to which the timestamp will be appended.\n    :return: string of the content with timestamp links added."
    },
    {
      "id": "fileops.add_timestamp_links",
      "label": "add_timestamp_links",
      "group": "function",
      "module": "fileops.py",
      "submodule": "TIMESTAMP LINKS",
      "def": "def add_timestamp_links(file_path):",
      "line": 1281,
      "docstring": "Adds markdown timestamp links and overwrites the file.\n\n    :param file_path: string, the path to the file where timestamp links will be added.\n    :return: none"
    },
    {
      "id": "fileops.count_num_instances",
      "label": "count_num_instances",
      "group": "function",
      "module": "fileops.py",
      "submodule": "FIND AND REPLACE",
      "def": "def count_num_instances(file_path, find_str):",
      "line": 1299,
      "docstring": "Counts the number of instances of a specific string in the text of the file.\n    Is case-sensitive.\n\n    :param file_path: string, the path to the file where the search will be performed.\n    :param find_str: string, the string to find in the file content.\n    :return: int, the number of instances found, or zero if no instances are found."
    },
    {
      "id": "fileops.find_and_replace_pairs",
      "label": "find_and_replace_pairs",
      "group": "function",
      "module": "fileops.py",
      "submodule": "FIND AND REPLACE",
      "def": "def find_and_replace_pairs(file_path, find_replace_pairs, use_regex=False):",
      "line": 1312,
      "docstring": "Finds and replaces multiple specified strings or regex patterns in the file and overwrites the original file.\n\n    :param file_path: string, the path to the file where the find and replace operations will be performed.\n    :param find_replace_pairs: list of tuples, each containing a string or regex pattern to be found and a string to replace it with.\n    :param use_regex: boolean for whether to use regex patterns for finding. Default is False (use exact string matching).\n    :return: int, the total number of replacements made."
    },
    {
      "id": "fileops.parse_csv_for_find_replace",
      "label": "parse_csv_for_find_replace",
      "group": "function",
      "module": "fileops.py",
      "submodule": "FIND AND REPLACE",
      "def": "def parse_csv_for_find_replace(csv_file):",
      "line": 1339,
      "docstring": "Parses a CSV file to extract find and replace pairs.\n    Returns a list of the find_"
    },
    {
      "id": "fileops.find_and_replace_from_csv",
      "label": "find_and_replace_from_csv",
      "group": "function",
      "module": "fileops.py",
      "submodule": "FIND AND REPLACE",
      "def": "def find_and_replace_from_csv(folder_path, find_replace_csv, suffixpat_include=None, verbose=False):",
      "line": 1360,
      "docstring": "Applies find and replace operations on all files in a specified folder based on pairs defined in a CSV file.\n    Overwrite is fixed at 'yes' so you have to copy the files before running.\n    IMPORTANT csv file cannot be in the same folder if suffixpat_include=None\n    Good practice is to always include a suffixpat_include even if those are the only type of file in the folder.\n\n    :param folder_path: string, the path to the folder where the files are located.\n    :param find_replace_csv: string, the path to the CSV file containing find and replace pairs.\n    :param suffixpat_include: string, the suffix pattern that included files must have. If None, all files will be processed.\n    :param verbose: boolean, if True prints verbose messages.\n    :return: None"
    },
    {
      "id": "fileops.get_heading_level",
      "label": "get_heading_level",
      "group": "function",
      "module": "fileops.py",
      "submodule": "HEADINGS",
      "def": "def get_heading_level(heading):",
      "line": 1389,
      "docstring": "Determines the level of a markdown heading.\n\n    :param heading: string, the markdown heading including '#' characters.\n    :return: int, the level of the heading."
    },
    {
      "id": "fileops.get_heading_pattern",
      "label": "get_heading_pattern",
      "group": "function",
      "module": "fileops.py",
      "submodule": "HEADINGS",
      "def": "def get_heading_pattern(heading):",
      "line": 1397,
      "docstring": "Creates a regex pattern to match a heading and its content, including subheadings.\n\n    :param heading: string, the markdown heading including '#' characters.\n    :return: compiled regex pattern or None if heading is empty"
    },
    {
      "id": "fileops.find_heading_text",
      "label": "find_heading_text",
      "group": "function",
      "module": "fileops.py",
      "submodule": "HEADINGS",
      "def": "def find_heading_text(full_text, heading):",
      "line": 1419,
      "docstring": "Finds the heading text, including its subheadings, inclusive of the heading itself.\n\n    :param text: string, the text to search in.\n    :param heading: string, the markdown heading to find.\n    :return: tuple (start_index, end_index) or None if not found."
    },
    {
      "id": "fileops.get_heading",
      "label": "get_heading",
      "group": "function",
      "module": "fileops.py",
      "submodule": "HEADINGS",
      "def": "def get_heading(file_path, heading):",
      "line": 1434,
      "docstring": "Extracts the markdown heading and its associated text from a file, including any subheadings of equal or lower order.\n    Uses the complete text and does not parse the metadata and content sections.\n\n    :param file_path: string, the path to the file to be read.\n    :param heading: string, the markdown heading to be extracted, including the '#' characters and the following space.\n    :return: string, the markdown heading and its associated text, including any subheadings of equal or lower order."
    },
    {
      "id": "fileops.set_heading",
      "label": "set_heading",
      "group": "function",
      "module": "fileops.py",
      "submodule": "HEADINGS",
      "def": "def set_heading(file_path, new_text, heading):",
      "line": 1449,
      "docstring": "Sets the heading and following text associated with a markdown heading and overwrites the file.\n    Replaces if the heading already exists. Adds if it does not exist.\n    New line characters must be included in the new_text (e.g., '\\nHere's new text\\n\\n').\n    \n    :param file_path: string, the path to the file where the heading text will be set.\n    :param new_text: string, the new text to be associated with the markdown heading, inclusive of newlines.\n    :param heading: string, the markdown heading whose text will be set, including the '#' characters and the following space.\n    :return: None"
    },
    {
      "id": "fileops.delete_heading",
      "label": "delete_heading",
      "group": "function",
      "module": "fileops.py",
      "submodule": "HEADINGS",
      "def": "def delete_heading(file_path, heading):",
      "line": 1494,
      "docstring": "Deletes the specified markdown heading and its following text, including subheadings, and overwrites the file.\n    If the heading does not exist, a warning is issued and no action is taken.\n\n    :param file_path: string of the path to the file from which the heading will be deleted.\n    :param heading: string of the markdown heading to be deleted, including the '#' characters and the following space."
    },
    {
      "id": "fileops.append_heading_to_file",
      "label": "append_heading_to_file",
      "group": "function",
      "module": "fileops.py",
      "submodule": "HEADINGS",
      "def": "def append_heading_to_file(source_file_path, target_file_path, heading, include_filename=True):",
      "line": 1514,
      "docstring": "Appends the text under a specified heading from a source file to a target file, optionally including the source filename as a heading.\n\n    :param source_file_path: string of the path to the source file.\n    :param target_file_path: string of the path to the target file where the text will be appended.\n    :param heading: string of the markdown heading to be appended.\n    :param include_filename: boolean indicating whether to include the source filename as a heading. Defaults to True.\n    :return: string of the appended text or None if the heading is not found in the source file."
    },
    {
      "id": "fileops.create_new_file_from_heading",
      "label": "create_new_file_from_heading",
      "group": "function",
      "module": "fileops.py",
      "submodule": "HEADINGS",
      "def": "def create_new_file_from_heading(file_path, heading, suffix_new='_headingonly', remove_heading=False):",
      "line": 1548,
      "docstring": "Extracts text under a specified heading from a file and writes it to a new file with a specified suffix.\n\n    :param file_path: string of the path to the file.\n    :param heading: string of the markdown heading to be processed.\n    :param suffix_new: string of the suffix to be appended to the new file. Defaults to '_headingonly'.\n    :param remove_heading: boolean indicating whether to remove the heading from the extracted text. Defaults to False.\n    :return: string of the path to the newly created file."
    },
    {
      "id": "fileops.set_metadata_field",
      "label": "set_metadata_field",
      "group": "function",
      "module": "fileops.py",
      "submodule": "METADATA",
      "def": "def set_metadata_field(metadata, field, value):",
      "line": 1575,
      "docstring": "Sets or updates a metadata field with a given value.\n\n    :param metadata: string, the metadata from which a metadata field is to be set or updated.\n    :param field: string, the metadata field to be set or updated without the : and space.\n    :param value: string, the value to be set for the metadata field.\n    :return: string, the updated metadata with the set or updated metadata field."
    },
    {
      "id": "fileops.remove_metadata_field",
      "label": "remove_metadata_field",
      "group": "function",
      "module": "fileops.py",
      "submodule": "METADATA",
      "def": "def remove_metadata_field(metadata, field):",
      "line": 1618,
      "docstring": "Removes a specified metadata field from the metadata.\n\n    :param metadata: string, the metadata from which a metadata field is to be removed.\n    :param field: string, the metadata field to be removed.\n    :return: string, the updated metadata with the removed metadata field."
    },
    {
      "id": "fileops.create_initial_metadata",
      "label": "create_initial_metadata",
      "group": "function",
      "module": "fileops.py",
      "submodule": "METADATA",
      "def": "def create_initial_metadata():",
      "line": 1632,
      "docstring": "Creates an initial metadata for a file.\n\n    :return: string, the initial metadata for a file."
    },
    {
      "id": "fileops.set_last_updated",
      "label": "set_last_updated",
      "group": "function",
      "module": "fileops.py",
      "submodule": "METADATA",
      "def": "def set_last_updated(metadata, new_last_updated_value, use_today=True):",
      "line": 1640,
      "docstring": "Updates the 'last updated' metadata field in the metadata with a new value.\n\n    :param metadata: string, the metadata from which the 'last updated' metadata field is to be updated.\n    :param new_last_updated_value: string, the new value for the 'last updated' metadata field.\n    :param use_today: boolean, if True, today's date is prepended to the new_last_updated_value. Default is True.\n    :return: string, the updated metadata with the new 'last updated' value."
    },
    {
      "id": "fileops.read_metadata_field_from_file",
      "label": "read_metadata_field_from_file",
      "group": "function",
      "module": "fileops.py",
      "submodule": "METADATA",
      "def": "def read_metadata_field_from_file(file_path, field):",
      "line": 1653,
      "docstring": "Reads a specific metadata field from a file.\n\n    :param file_path: string, the path to the file to be read.\n    :param field: string, the metadata field to be read from the file.\n    :return: tuple, the line number of the field and the value of the field."
    },
    {
      "id": "fileops.set_metadata_fields_from_csv",
      "label": "set_metadata_fields_from_csv",
      "group": "function",
      "module": "fileops.py",
      "submodule": "METADATA",
      "def": "def set_metadata_fields_from_csv(folder_path, csv_file_path, suffix_extension):",
      "line": 1678,
      "docstring": "Sets metadata fields for all files in a folder based on a CSV file.\n\n    :param folder_path: string of the path to the folder containing the files.\n    :param csv_file_path: string of the path to the CSV file with metadata fields and values.\n    :param suffix_extension: string of the file extension to be appended to file bases.\n    :return: None, but prints the number of files processed successfully and the total in the CSV excluding empty rows."
    },
    {
      "id": "fileops.create_csv_from_fields",
      "label": "create_csv_from_fields",
      "group": "function",
      "module": "fileops.py",
      "submodule": "METADATA",
      "def": "def create_csv_from_fields(folder_path, fields):",
      "line": 1725,
      "docstring": "Generates a CSV file of input fields from all markdown files in a folder.\n    Fields can be 2 formats, 1) generic, usually metadata (ending with a ':') or 2) markdown headings (starting with a '#').\n    Generic fields extract single-line values, while heading fields capture all content under the heading.\n\n    :param folder_path: string, the path to the folder containing the markdown files.\n    :param fields: list, the fields to be extracted from the markdown files.\n    :return: string of the path to the created csv file."
    },
    {
      "id": "fileops.create_csv_matrix_from_triples",
      "label": "create_csv_matrix_from_triples",
      "group": "function",
      "module": "fileops.py",
      "submodule": "METADATA",
      "def": "def create_csv_matrix_from_triples(triples_text, target_file_path):",
      "line": 1785,
      "docstring": "Converts multiline text of triples into a csv matrix file where each entry is the number for that row and column.\n\n    :param triples_text: string of multiline text containing rows of data separated by newlines, each row containing two strings and a number separated by commas.\n    :param target_file_path: string of the path to the target csv file.\n    :return: string of the path to the created csv file."
    },
    {
      "id": "transcribe.download_mp3_from_youtube",
      "label": "download_mp3_from_youtube",
      "group": "function",
      "module": "transcribe.py",
      "submodule": "YOUTUBE",
      "def": "def download_mp3_from_youtube(url, output_title='downloaded_audio'):",
      "line": 31,
      "docstring": "Downloads an audio file from a YouTube URL and saves it as an mp3 file. Uses yt_dlp package.\n\n    :param url: string of the YouTube URL from which to download the audio.\n    :param output_title: string of the title to save the downloaded mp3 file as. defaults to 'downloaded_audio'.\n    :return: string of the path to the saved mp3 file."
    },
    {
      "id": "transcribe.get_youtube_title_length",
      "label": "get_youtube_title_length",
      "group": "function",
      "module": "transcribe.py",
      "submodule": "YOUTUBE",
      "def": "def get_youtube_title_length(url):",
      "line": 59,
      "docstring": "Retrieves the title and duration of a youtube video in a formatted timestamp. Uses yt_dlp package.\n\n    :param url: string of the youtube url to retrieve information from.\n    :return: tuple containing the video title and its duration as a string in a formatted timestamp."
    },
    {
      "id": "transcribe.download_link_list_to_mp3s",
      "label": "download_link_list_to_mp3s",
      "group": "function",
      "module": "transcribe.py",
      "submodule": "YOUTUBE",
      "def": "def download_link_list_to_mp3s(links, audio_inbox_path=\"data/audio_inbox\"):  # NO CALLERS (3-3 RT)",
      "line": 84,
      "docstring": "Downloads a list of youtube links as mp3 files to a specified directory and stores the link-title pairs. Uses yt_dlp package.\n    Calls download_mp3_from_youtube\n\n    :param links: list of youtube links to be downloaded.\n    :param audio_inbox_path: string of the directory path where the audio files will be saved.\n    :return: dictionary mapping each youtube link to its corresponding title."
    },
    {
      "id": "transcribe.download_youtube_subtitles_url",
      "label": "download_youtube_subtitles_url",
      "group": "function",
      "module": "transcribe.py",
      "submodule": "YOUTUBE",
      "def": "def download_youtube_subtitles_url(subtitle_url): # DS, cat 1, omit unittests since called by next function",
      "line": 100,
      "docstring": "Downloads and extracts subtitle text from a given YouTube subtitle URL.\n    Helper function to that is called from get_youtube_subtitles\n    \n    :param subtitle_url: string of the url from which subtitles are to be downloaded.\n    :return: string of the extracted subtitle text, spaces between segments and stripped of new lines."
    },
    {
      "id": "transcribe.get_youtube_subtitles",
      "label": "get_youtube_subtitles",
      "group": "function",
      "module": "transcribe.py",
      "submodule": "YOUTUBE",
      "def": "def get_youtube_subtitles(url):",
      "line": 121,
      "docstring": "Retrieves English subtitles for a given YouTube video URL if available. Uses yt_dlp package.\n    \n    :param url: string of the youtube video url.\n    :return: subtitles as a string if found, otherwise None."
    },
    {
      "id": "transcribe.get_youtube_all",
      "label": "get_youtube_all",
      "group": "function",
      "module": "transcribe.py",
      "submodule": "YOUTUBE",
      "def": "def get_youtube_all(url):",
      "line": 152,
      "docstring": "Retrieves all available information from a YouTube video URL, including title, length, chapters, description, and transcript. Uses yt_dlp package.\n    \n    :param url: string of the youtube video url.\n    :return: dictionary with video details or None if the URL is invalid."
    },
    {
      "id": "transcribe.is_valid_youtube_url",
      "label": "is_valid_youtube_url",
      "group": "function",
      "module": "transcribe.py",
      "submodule": "YOUTUBE",
      "def": "def is_valid_youtube_url(url):",
      "line": 233,
      "docstring": "Determine if a string of url is a valid YouTube URL by attempting to fetch video info using the yt_dlp package.\n\n    :param url: string of url to be validated.\n    :return: boolean where true if the url is valid, false otherwise."
    },
    {
      "id": "transcribe.create_youtube_md",
      "label": "create_youtube_md",
      "group": "function",
      "module": "transcribe.py",
      "submodule": "YOUTUBE",
      "def": "def create_youtube_md(url, title_or_path=None):  # unittests 3 APICALL + 1 APIMOCK",
      "line": 259,
      "docstring": "Generates a markdown file containing metadata, chapters, description, and transcript from a YouTube video.\n\n    :param url: string of the url to be processed.\n    :param title_or_path: string of the title or path for the markdown file, defaults to None.\n    :return: string of the path to the created markdown file."
    },
    {
      "id": "transcribe.create_youtube_md_from_file_link",
      "label": "create_youtube_md_from_file_link",
      "group": "function",
      "module": "transcribe.py",
      "submodule": "YOUTUBE",
      "def": "def create_youtube_md_from_file_link(md_file_path):",
      "line": 299,
      "docstring": "Creates a YouTube markdown file from a given file path by extracting the YouTube link from the file's metadata.\n    \n    :param md_file_path: string of the path to the markdown file containing the YouTube link in its metadata.\n    :return: string of the path to the created YouTube markdown file."
    },
    {
      "id": "transcribe.extract_feature_from_youtube_md",
      "label": "extract_feature_from_youtube_md",
      "group": "function",
      "module": "transcribe.py",
      "submodule": "YOUTUBE",
      "def": "def extract_feature_from_youtube_md(yt_md_file_path, feature):",
      "line": 322,
      "docstring": "Extracts a specified feature from a YouTube markdown file and returns it as a string.\n\n    :param yt_md_file_path: string of the path to the markdown file from which the feature is to be extracted.\n    :param feature: string of the feature to be extracted (e.g., 'chapters', 'description', 'transcript').\n    :return: string of the extracted text under the specified feature"
    },
    {
      "id": "transcribe.test_deepgram_client",
      "label": "test_deepgram_client",
      "group": "function",
      "module": "transcribe.py",
      "submodule": "DEEPGRAM AND JSON",
      "def": "def test_deepgram_client():  # omit unittests",
      "line": 356,
      "docstring": "Tests the Deepgram client initialization with the provided API key and prints a success or failure message.\n    Raises ValueError if test fails."
    },
    {
      "id": "transcribe.get_media_length",
      "label": "get_media_length",
      "group": "function",
      "module": "transcribe.py",
      "submodule": "DEEPGRAM AND JSON",
      "def": "def get_media_length(file_path_or_url):",
      "line": 369,
      "docstring": "Retrieves the length (duration) of a media file or a YouTube video.\n    For a local file, it returns the duration in seconds.\n    For a YouTube video, it returns the duration in our tuned timestamp format.\n\n    :param file_path_or_url: Path to a local media file or a URL to a YouTube video.\n    :return: length (duration) of the media in seconds (for local files) or in our tuned timestamp format (for YouTube videos)."
    },
    {
      "id": "transcribe.add_link_to_json",
      "label": "add_link_to_json",
      "group": "function",
      "module": "transcribe.py",
      "submodule": "DEEPGRAM AND JSON",
      "def": "def add_link_to_json(json_file_path, link):",
      "line": 401,
      "docstring": "Add a hyperlink to the JSON file under the 'metadata' section.\n\n    :param json_file_path: string, the path to the JSON file to be modified.\n    :param link: string, the hyperlink to be added to the JSON file.\n    :return: tuple, the path to the modified JSON file and None if successful, or None and an exception if an error occurs."
    },
    {
      "id": "transcribe.get_link_from_json",
      "label": "get_link_from_json",
      "group": "function",
      "module": "transcribe.py",
      "submodule": "DEEPGRAM AND JSON",
      "def": "def get_link_from_json(json_file_path):",
      "line": 428,
      "docstring": "Retrieve the hyperlink from the 'metadata' section of a JSON file.\n\n    :param json_file_path: string, the path to the JSON file from which the hyperlink is to be retrieved.\n    :return: string or None, the hyperlink if found in the JSON file's 'metadata' section, otherwise None."
    },
    {
      "id": "transcribe.transcribe_deepgram",
      "label": "transcribe_deepgram",
      "group": "function",
      "module": "transcribe.py",
      "submodule": "DEEPGRAM AND JSON",
      "def": "def transcribe_deepgram(audio_file_path, model):",
      "line": 445,
      "docstring": "Calls the Deepgram API to transcribe the given audio file using the specified Deepgram model.\n\n    :param audio_file_path: path to the audio file to be transcribed.\n    :param model: the Deepgram model to use for transcription, accpets deepgram api call model or our suffix version (see below).\n    :return: a dictionary containing the transcription results."
    },
    {
      "id": "transcribe.transcribe_deepgram_sdk_prerecorded",
      "label": "transcribe_deepgram_sdk_prerecorded",
      "group": "function",
      "module": "transcribe.py",
      "submodule": "DEEPGRAM AND JSON",
      "def": "def transcribe_deepgram_sdk_prerecorded(audio_file_path, model):",
      "line": 526,
      "docstring": "Calls the Deepgram API to transcribe the given audio file using the specified Deepgram model, utilizing the SDK.\n\n    :param audio_file_path: path to the audio file to be transcribed.\n    :param model: the Deepgram model to use for transcription.\n    :return: path to the JSON file containing the transcription results."
    },
    {
      "id": "transcribe.transcribe_deepgram_callback",
      "label": "transcribe_deepgram_callback",
      "group": "function",
      "module": "transcribe.py",
      "submodule": "DEEPGRAM AND JSON",
      "def": "def transcribe_deepgram_callback(audio_file_path, model, callback_url):",
      "line": 600,
      "docstring": "Transcribes the given audio file using the specified Deepgram model asynchronously with a callback URL.\n\n    :param audio_file_path: path to the audio file to be transcribed.\n    :param model: the Deepgram model to use for transcription.\n    :param callback_url: URL to which Deepgram will send the transcription results.\n    :return: Request ID from Deepgram indicating that the file has been accepted for processing."
    },
    {
      "id": "transcribe.transcribe_deepgram_callback2",
      "label": "transcribe_deepgram_callback2",
      "group": "function",
      "module": "transcribe.py",
      "submodule": "DEEPGRAM AND JSON",
      "def": "def transcribe_deepgram_callback2(audio_file_path, model, callback_url):",
      "line": 688,
      "docstring": "Transcribes the given audio file using the specified Deepgram model asynchronously with a callback URL.\n\n    :param audio_file_path: path to the audio file to be transcribed.\n    :param model: the Deepgram model to use for transcription.\n    :param callback_url: URL to which Deepgram will send the transcription results.\n    :return: Request ID from Deepgram indicating that the file has been accepted for processing."
    },
    {
      "id": "transcribe.transcribe_deepgram_OLD_fixhang",
      "label": "transcribe_deepgram_OLD_fixhang",
      "group": "function",
      "module": "transcribe.py",
      "submodule": "DEEPGRAM AND JSON",
      "def": "def transcribe_deepgram_OLD_fixhang(file_path, timeout_duration=1*60*60):",
      "line": 760,
      "docstring": "Retrieves the start time in seconds of a word from the transcription data at the given index.\n\n    :param data: dictionary of the transcription data.\n    :param index: integer of the index of the word to find the start time for.\n    :return: integer of the start time in seconds of the specified word, rounded down to the nearest whole number."
    },
    {
      "id": "transcribe.get_summary_start_seconds",
      "label": "get_summary_start_seconds",
      "group": "function",
      "module": "transcribe.py",
      "submodule": "DEEPGRAM AND JSON",
      "def": "def get_summary_start_seconds(data, index):",
      "line": 802,
      "docstring": "Retrieves the start time in seconds of a word from the transcription data at the given index.\n\n    :param data: dictionary of the transcription data.\n    :param index: integer of the index of the word to find the start time for.\n    :return: integer of the start time in seconds of the specified word, rounded down to the nearest whole number."
    },
    {
      "id": "transcribe.format_feature_segment",
      "label": "format_feature_segment",
      "group": "function",
      "module": "transcribe.py",
      "submodule": "DEEPGRAM AND JSON",
      "def": "def format_feature_segment(feature, segment, data):",
      "line": 814,
      "docstring": "Formats a segment of a feature with a timestamp and additional info.\n\n    :param feature: string, the feature being extracted.\n    :param segment: dict, the segment of the feature to be formatted.\n    :param data: dict, the JSON data from the Deepgram file.\n    :return: string, the formatted segment."
    },
    {
      "id": "transcribe.extract_feature_from_deepgram_json",
      "label": "extract_feature_from_deepgram_json",
      "group": "function",
      "module": "transcribe.py",
      "submodule": "DEEPGRAM AND JSON",
      "def": "def extract_feature_from_deepgram_json(json_file_path, feature):",
      "line": 860,
      "docstring": "Extract a specific feature section from a Deepgram JSON file and return it as a string.\n\n    :param json_file_path: string of the path to the JSON file from which the feature is to be extracted.\n    :param feature: string of the feature of the section to be extracted.\n    :return: string of the extracted text under the specified feature, preceded by the feature itself (no pound signs) and a blank line."
    },
    {
      "id": "transcribe.validate_transcript_json",
      "label": "validate_transcript_json",
      "group": "function",
      "module": "transcribe.py",
      "submodule": "DEEPGRAM AND JSON",
      "def": "def validate_transcript_json(json_file_path):",
      "line": 894,
      "docstring": "Validates the structure of a JSON file to ensure it contains specific keys and types.\n\n    :param json_file_path: string of the path to the JSON file to be validated.\n    :return: boolean, True if the JSON structure is as expected, False otherwise."
    },
    {
      "id": "transcribe.set_various_transcript_headings",
      "label": "set_various_transcript_headings",
      "group": "function",
      "module": "transcribe.py",
      "submodule": "DEEPGRAM AND JSON",
      "def": "def set_various_transcript_headings(file_path, feature, source):",
      "line": 942,
      "docstring": "Sets the transcript heading in a file based on the extracted feature from a specified source.\n\n    :param file_path: string of the path to the file where the heading is to be set.\n    :param feature: string of the feature to extract and use as the heading.\n    :param source: string of the source from which to extract the feature ('deepgram' or 'youtube').\n    :return: None."
    },
    {
      "id": "transcribe.extract_context",
      "label": "extract_context",
      "group": "function",
      "module": "transcribe.py",
      "submodule": "NUMERAL CONVERT",
      "def": "def extract_context(line, match, context_radius):",
      "line": 976,
      "docstring": "Extracts a context window around a regex match within a string of text.\n\n    :param line: string of text containing the match.\n    :param match: regex match object containing the start and end positions of the match within the line.\n    :param context_radius: integer specifying the number of words around the match to include in the context window.\n    :return: string of text representing the context window around the match."
    },
    {
      "id": "transcribe.print_num_exception",
      "label": "print_num_exception",
      "group": "function",
      "module": "transcribe.py",
      "submodule": "NUMERAL CONVERT",
      "def": "def print_num_exception(match_str, line_number, num_metadata_lines, printed_exceptions, exception_type, line):",
      "line": 1009,
      "docstring": "Prints a message for numbers that are excluded from conversion and records the message.\n\n    :param match_str: the string that matches the number to be excluded from conversion.\n    :param line_number: the current line number in the file being processed.\n    :param num_metadata_lines: the number of metadata lines in the file to adjust the actual line number.\n    :param printed_exceptions: a list of exception messages that have already been printed.\n    :param exception_type: the type of exception to be printed.\n    :param line: the current line of text being processed.\n    :return: None, but updates the printed_exceptions list with the new exception message if it hasn't been printed before."
    },
    {
      "id": "transcribe.get_previous_word",
      "label": "get_previous_word",
      "group": "function",
      "module": "transcribe.py",
      "submodule": "NUMERAL CONVERT",
      "def": "def get_previous_word(substring, start_index):",
      "line": 1031,
      "docstring": "Finds the word in a string that precedes the given start index.\n\n    :param substring: the string from which to extract the previous word.\n    :param start_index: the index in the string to start searching backward from.\n    :return: the word found before the start index, or an empty string if no word is found."
    },
    {
      "id": "transcribe.previous_word_exception",
      "label": "previous_word_exception",
      "group": "function",
      "module": "transcribe.py",
      "submodule": "NUMERAL CONVERT",
      "def": "def previous_word_exception(word, common_english_vocab, additional_exception_words):",
      "line": 1049,
      "docstring": "Determines if a word is an exception based on its presence in additional exceptions or English vocabulary.\n\n    :param word: the word to check for exception status.\n    :param common_english_vocab: a set of common English words to compare against.\n    :param additional_exception_words: a set of words that are always considered exceptions.\n    :return: True if the word is an exception, False otherwise."
    },
    {
      "id": "transcribe.convert_num_line_lowercase",
      "label": "convert_num_line_lowercase",
      "group": "function",
      "module": "transcribe.py",
      "submodule": "NUMERAL CONVERT",
      "def": "def convert_num_line_lowercase(line, num, num_str, line_number, num_metadata_lines, printed_exceptions, common_english_vocab):",
      "line": 1065,
      "docstring": "Converts numbers in a line of text to their lowercase word equivalents, skipping exceptions.\n\n    :param line: The line of text in which to convert numbers.\n    :param num: The numerical value to convert to words.\n    :param num_str: The string representation of the number to find in the line.\n    :param line_number: The current line number in the text being processed.\n    :param num_metadata_lines: The number of metadata lines in the text before the content.\n    :param printed_exceptions: A set to record exceptions that have been printed.\n    :param common_english_vocab: A set of common English vocabulary words.\n    :return: A tuple containing the modified line and the total number of substitutions made."
    },
    {
      "id": "transcribe.convert_num_line_capitalization",
      "label": "convert_num_line_capitalization",
      "group": "function",
      "module": "transcribe.py",
      "submodule": "NUMERAL CONVERT",
      "def": "def convert_num_line_capitalization(line, num, num_str):",
      "line": 1127,
      "docstring": "Capitalize the numeral word at the beginning of a sentence or after punctuation.\n\n    :param line: the line of text in which to perform capitalization.\n    :param num: the numerical value to convert to words.\n    :param num_str: the string representation of the number to find in the line.\n    :return: a tuple containing the modified line and the total number of substitutions made."
    },
    {
      "id": "transcribe.skip_speaker_line_with_timestamp",
      "label": "skip_speaker_line_with_timestamp",
      "group": "function",
      "module": "transcribe.py",
      "submodule": "NUMERAL CONVERT",
      "def": "def skip_speaker_line_with_timestamp(line):",
      "line": 1145,
      "docstring": "Determine if a line contains a single timestamp with max_words before the timestamp less that get_timestamp default val (8) and is therefore a speaker line to skip.\n\n    :param line: The line of text to be checked for a timestamp.\n    :return: boolean where True if a timestamp is found, otherwise False."
    },
    {
      "id": "transcribe.convert_num_lines",
      "label": "convert_num_lines",
      "group": "function",
      "module": "transcribe.py",
      "submodule": "NUMERAL CONVERT",
      "def": "def convert_num_lines(lines, num, num_str, num_metadata_lines, verbose, printed_exceptions):",
      "line": 1155,
      "docstring": "Converts numbers in lines of text to their word equivalents, handles capitalization, and skips lines with timestamps.\n    Takes both num and num_str as separate parameters to provide flexibility in how the function is called.\n    This design allows the caller to specify the string representation of the number 1 that should be searched for within the text lines,\n    which may not always be a straightforward string conversion of num.\n    For example, num could be an integer, but num_str could be a formatted string that represents the number\n    in a specific way within the text (e.g., '001' instead of '1', or '1st' for the ordinal form).\n    \n    :param lines: list of text lines to process.\n    :param num: the numerical value to convert to words.\n    :param num_str: the string representation of the number to find in the lines.\n    :param num_metadata_lines: the number of metadata lines in the document to adjust line numbering for output.\n    :param verbose: boolean indicating whether to print the conversion output.\n    :param printed_exceptions: list to record any exceptions encountered during processing.\n    :return: tuple containing the list of processed lines and the total number of substitutions made."
    },
    {
      "id": "transcribe.convert_numbers_in_content",
      "label": "convert_numbers_in_content",
      "group": "function",
      "module": "transcribe.py",
      "submodule": "NUMERAL CONVERT",
      "def": "def convert_numbers_in_content(content, num_limit, additional_numbers, num_metadata_lines, print_output):",
      "line": 1184,
      "docstring": "Converts numerical values in text content to their word equivalents, excluding lines with timestamps.\n\n    :param content: string containing the text content to be processed.\n    :param num_limit: integer representing the upper limit for numbers to convert.\n    :param additional_numbers: list of additional numbers to be converted outside the standard range.\n    :param num_metadata_lines: integer representing the number of metadata lines in the content.\n    :param print_output: boolean indicating whether to print the conversion output.\n    :return: tuple containing the converted content as a string and the total number of substitutions made."
    },
    {
      "id": "transcribe.convert_ordinals_in_content",
      "label": "convert_ordinals_in_content",
      "group": "function",
      "module": "transcribe.py",
      "submodule": "NUMERAL CONVERT",
      "def": "def convert_ordinals_in_content(content, punct_capitalization):",
      "line": 1204,
      "docstring": "Converts ordinal numbers in a string of text to their word equivalents and capitalizes words following specified punctuation.\n\n    :param content: string of text containing ordinal numbers and punctuation.\n    :param punct_capitalization: list of punctuation characters after which the following word should be capitalized.\n    :return: string of text with ordinal numbers converted and words capitalized as specified."
    },
    {
      "id": "transcribe.convert_nums_to_words",
      "label": "convert_nums_to_words",
      "group": "function",
      "module": "transcribe.py",
      "submodule": "NUMERAL CONVERT",
      "def": "def convert_nums_to_words(file_path, verbose=False):",
      "line": 1228,
      "docstring": "Converts numerals in the content of a file to their corresponding words, appends a specified suffix to the filename, and creates a new file with the converted content.\n\n    :param file_path: string of the path to the original file.\n    :param verbose: boolean for printing verbose messages. Defaults to False.\n    :return: string of the path to the newly created file with the converted content."
    },
    {
      "id": "transcribe.read_speaker_names_from_json",
      "label": "read_speaker_names_from_json",
      "group": "function",
      "module": "transcribe.py",
      "submodule": "SPEAKER NAMES",
      "def": "def read_speaker_names_from_json(json_file_path):",
      "line": 1254,
      "docstring": "Reads speaker names from a JSON file's metadata, which have been inserted by us and are not in the raw deepgram json files.\n\n    :param json_file_path: string of the path to the JSON file.\n    :return: list of speaker names if they exist, otherwise an empty list."
    },
    {
      "id": "transcribe.write_speaker_names_to_json",
      "label": "write_speaker_names_to_json",
      "group": "function",
      "module": "transcribe.py",
      "submodule": "SPEAKER NAMES",
      "def": "def write_speaker_names_to_json(json_file_path, speaker_names, verbose=False):",
      "line": 1268,
      "docstring": "Writes speaker names to a JSON file's metadata. Overwrites file.\n\n    :param json_file_path: string of the path to the JSON file.\n    :param speaker_names: list of strings containing speaker names.\n    :return: None."
    },
    {
      "id": "transcribe.find_unassigned_speakers",
      "label": "find_unassigned_speakers",
      "group": "function",
      "module": "transcribe.py",
      "submodule": "SPEAKER NAMES",
      "def": "def find_unassigned_speakers(md_file_path, verbose=False):",
      "line": 1307,
      "docstring": "Identifies speakers in the markdown file who do not have assigned names.\n    This is determined by if the line has a valid timestamp and then looking for 'Speaker X' before the timestamp.\n\n    :param md_file_path: string of the path to the markdown file.\n    :return: list of strings of unassigned speaker names, or None if all speakers are assigned."
    },
    {
      "id": "transcribe.propagate_speaker_names_throughout_md",
      "label": "propagate_speaker_names_throughout_md",
      "group": "function",
      "module": "transcribe.py",
      "submodule": "SPEAKER NAMES",
      "def": "def propagate_speaker_names_throughout_md(md_file_path, input_speaker_names=None):",
      "line": 1349,
      "docstring": "Propagates speaker names throughout a markdown file based on provided input names or existing assignments.\n\n    :param md_file_path: string of the path to the markdown file.\n    :param input_speaker_names: list of tuples with speaker numbers and names, if available.\n    :return: list of tuples with speaker numbers and names after propagation."
    },
    {
      "id": "transcribe.iterate_input_speaker_names",
      "label": "iterate_input_speaker_names",
      "group": "function",
      "module": "transcribe.py",
      "submodule": "SPEAKER NAMES",
      "def": "def iterate_input_speaker_names(md_file_path, input_speaker_names=None):",
      "line": 1405,
      "docstring": "Iterates over input speaker names and updates the markdown file until the user decides to exit.\n\n    :param md_file_path: string of the path to the markdown file.\n    :param input_speaker_names: list of tuples with speaker numbers and names, if available.\n    :return: list of tuples with speaker numbers and names after all iterations."
    },
    {
      "id": "transcribe.assign_speaker_names",
      "label": "assign_speaker_names",
      "group": "function",
      "module": "transcribe.py",
      "submodule": "SPEAKER NAMES",
      "def": "def assign_speaker_names(md_file_path):",
      "line": 1451,
      "docstring": "Assigns speaker names to markdown file by reading from a corresponding JSON file, updating, and writing back to the json if changed.\n    Prompts the user iteratively through assigning the names.\n    \n    :param md_file_path: string of the path to the markdown file.\n    :return: None"
    },
    {
      "id": "transcribe.create_transcript_md_from_json",
      "label": "create_transcript_md_from_json",
      "group": "function",
      "module": "transcribe.py",
      "submodule": "TRANSCRIBE WRAPPER",
      "def": "def create_transcript_md_from_json(json_file_path, combine_segs=True):",
      "line": 1477,
      "docstring": "Creates a markdown transcript from a JSON file containing Deepgram transcription data.\n    If combine_segs is True, combines consecutive segments from the same speaker.\n\n    :param json_file_path: string of the path to the json file containing transcription data.\n    :param combine_segs: boolean indicating whether to combine consecutive segments from the same speaker.\n    :return: string of the path to the created markdown file or None if the json file is not valid."
    },
    {
      "id": "transcribe.process_deepgram_transcription",
      "label": "process_deepgram_transcription",
      "group": "function",
      "module": "transcribe.py",
      "submodule": "TRANSCRIBE WRAPPER",
      "def": "def process_deepgram_transcription(title, link, model, audio_inbox_path=\"data/audio_inbox\"):  # unittests 1 TEMP SKIPPED",
      "line": 1544,
      "docstring": "Processes a Deepgram transcription from a YouTube video link by downloading the audio, transcribing it, and creating a markdown transcript.\n\n    :param title: the title of the video used to name the downloaded audio file.\n    :param link: the YouTube link to the video to be transcribed.\n    :param model: the Deepgram model used for transcription.\n    :param audio_inbox_path: the directory path where the audio file will be downloaded.\n    :return: the path to the created markdown file or None if transcription fails."
    },
    {
      "id": "transcribe.process_deepgram_transcription_from_audio_file",
      "label": "process_deepgram_transcription_from_audio_file",
      "group": "function",
      "module": "transcribe.py",
      "submodule": "TRANSCRIBE WRAPPER",
      "def": "def process_deepgram_transcription_from_audio_file(audio_file_path, link, model):  # unittests 1 TEMP SKIPPED",
      "line": 1574,
      "docstring": "Transcribes an audio file using the Deepgram service, adds the YouTube link to the transcription, creates a markdown transcript, and assigns speaker names.\n\n    :param audio_file_path: string of the path to the audio file to be transcribed.\n    :param link: string of the youtube link to be added to the transcription json.\n    :param model: string of the deepgram model to be used for transcription.\n    :return: string of the path to the markdown file with the completed transcription or None if transcription fails."
    },
    {
      "id": "transcribe.process_multiple_videos",
      "label": "process_multiple_videos",
      "group": "function",
      "module": "transcribe.py",
      "submodule": "TRANSCRIBE WRAPPER",
      "def": "def process_multiple_videos(videos_to_process, model='nova-2-general', bool_youtube=True):  # unittests 1 MOCK",
      "line": 1597,
      "docstring": "Processes multiple videos by transcribing them and creating YouTube markdown files if bool_youtube is True.\n\n    :param videos_to_process: list of tuples containing the title and link of each video to be processed.\n    :param model: string of the deepgram model to be used for transcription. Defaults to 'enhmeet' (deepgram enhanced-meeting) model.\n    :param bool_youtube: boolean indicating whether to create YouTube markdown files. Defaults to True.\n    :return: None"
    },
    {
      "id": "llm.pretty_print_function",
      "label": "pretty_print_function",
      "group": "function",
      "module": "llm.py",
      "submodule": "PRINT AND TOKENS",
      "def": "def pretty_print_function(messages, tools, print_prompts=False, print_input=True, verbose=False):",
      "line": 43,
      "docstring": "Prints messages with role-specific colors and separates function details for clarity.\n\n    :param messages: list of dictionaries containing message role and content\n    :params tools: list of tools, each containing function details, passed to pretty_print_function_descriptions\n    :param print_prompts: boolean of whether to print the system prompt and function parameter descriptions, defaults to False\n    :param print_input: boolean of whether to print the user input, defaults to True\n    :return: a list of the print strings as [print_str_prompts, print_str_input, print_str_responses]"
    },
    {
      "id": "llm.pretty_print_function_descriptions",
      "label": "pretty_print_function_descriptions",
      "group": "function",
      "module": "llm.py",
      "submodule": "PRINT AND TOKENS",
      "def": "def pretty_print_function_descriptions(tools, print_color):",
      "line": 111,
      "docstring": "Print descriptions of functions and their properties from a list of tools.\n\n    :param tools: a list of tools, each containing function details\n    :return: a string of function names and descriptions, including properties"
    },
    {
      "id": "llm.count_tokens",
      "label": "count_tokens",
      "group": "function",
      "module": "llm.py",
      "submodule": "PRINT AND TOKENS",
      "def": "def count_tokens(input_string):  # no unittests",
      "line": 137,
      "docstring": "Counts the number of tokens in a given string using the 'cl100k_base' encoding.\n\n    :param input_string: string of text to be tokenized.\n    :return: integer representing the number of tokens in the input string."
    },
    {
      "id": "llm.cost_llm_on_file",
      "label": "cost_llm_on_file",
      "group": "function",
      "module": "llm.py",
      "submodule": "PRINT AND TOKENS",
      "def": "def cost_llm_on_file(file_path, prompt, model, token_cost_dict, verbose=False, chunking_function=None, chunking_function_args=(), output_tokens_ratio=1, output_tokens_fixed=0):  # no unittests",
      "line": 147,
      "docstring": "Calculates the cost of processing a file using a language model, based on the number of input and output tokens.\n\n    :param file_path: string of the path to the file to be processed.\n    :param prompt: string of the prompt to be used for the language model.\n    :param model: string of the name of the language model to be used.\n    :param token_cost_dict: dictionary containing the cost per token for the input and output of the model.\n    :param chunking_function: function to be used for chunking the file, defaults to None.\n    :param chunking_function_args: tuple of arguments to be passed to the chunking function, defaults to an empty tuple.\n    :param output_tokens_ratio: ratio of input tokens to output tokens, defaults to 1.\n    :param output_tokens_fixed: fixed number of output tokens per chunk, defaults to 0.\n    :return: tuple of total input cost, total output cost, and total cost."
    },
    {
      "id": "llm.default_chunking",
      "label": "default_chunking",
      "group": "function",
      "module": "llm.py",
      "submodule": "PRINT AND TOKENS",
      "def": "def default_chunking(file_path):",
      "line": 165,
      "docstring": "Calculates the cost of processing a corpus using a language model, based on the number of input and output tokens.\n\n    :param corpus_path: string of the path to the corpus to be processed.\n    :param prompt: string of the prompt to be used for the language model.\n    :param model: string of the name of the language model to be used.\n    :param token_cost_dict: dictionary containing the cost per token for the input and output of the model.\n    :param chunking_function: function to be used for chunking the file, defaults to None.\n    :param chunking_function_args: tuple of arguments to be passed to the chunking function, defaults to an empty tuple.\n    :param output_tokens_ratio: ratio of input tokens to output tokens, defaults to 1.\n    :param output_tokens_fixed: fixed number of output tokens per chunk, defaults to 0.\n    :param suffix_include: string of the suffix that included files must have, defaults to None.\n    :param suffix_exclude: string of the suffix that files must not have to be included, defaults to None.\n    :param include_subfolders: boolean indicating whether to include files from subfolders, defaults to False.\n    :return: tuple of total input cost, total output cost, and total cost for the entire corpus."
    },
    {
      "id": "llm.cost_llm_on_corpus",
      "label": "cost_llm_on_corpus",
      "group": "function",
      "module": "llm.py",
      "submodule": "PRINT AND TOKENS",
      "def": "def cost_llm_on_corpus(corpus_path, prompt, model, token_cost_dict, verbose=False, suffix_include=None, suffix_exclude=None, include_subfolders=False, chunking_function=None, chunking_function_args=(), output_tokens_ratio=1, output_tokens_fixed=0):",
      "line": 203,
      "docstring": "Calculates the cost of processing a corpus using a language model, based on the number of input and output tokens.\n\n    :param corpus_path: string of the path to the corpus to be processed.\n    :param prompt: string of the prompt to be used for the language model.\n    :param model: string of the name of the language model to be used.\n    :param token_cost_dict: dictionary containing the cost per token for the input and output of the model.\n    :param chunking_function: function to be used for chunking the file, defaults to None.\n    :param chunking_function_args: tuple of arguments to be passed to the chunking function, defaults to an empty tuple.\n    :param output_tokens_ratio: ratio of input tokens to output tokens, defaults to 1.\n    :param output_tokens_fixed: fixed number of output tokens per chunk, defaults to 0.\n    :param suffix_include: string of the suffix that included files must have, defaults to None.\n    :param suffix_exclude: string of the suffix that files must not have to be included, defaults to None.\n    :param include_subfolders: boolean indicating whether to include files from subfolders, defaults to False.\n    :return: tuple of total input cost, total output cost, and total cost for the entire corpus."
    },
    {
      "id": "llm.add_token_counts_to_headings",
      "label": "add_token_counts_to_headings",
      "group": "function",
      "module": "llm.py",
      "submodule": "PRINT AND TOKENS",
      "def": "def add_token_counts_to_headings(text):",
      "line": 241,
      "docstring": "Adds token counts to markdown headings in the given text.\n\n    :param text: string, the text content to process.\n    :return: string, the text with token counts added to headings."
    },
    {
      "id": "llm.get_line_numbers_with_match",
      "label": "get_line_numbers_with_match",
      "group": "function",
      "module": "llm.py",
      "submodule": "SPLIT FILES",
      "def": "def get_line_numbers_with_match(file_path, match_str):",
      "line": 275,
      "docstring": "Retrieve line numbers from a file where the line matches a given string exactly after stripping.\n\n    :param file_path: path to the file to be searched\n    :param match_str: string of text to match on each line\n    :return: list of line numbers where the match_str is found"
    },
    {
      "id": "llm.get_speaker_segments",
      "label": "get_speaker_segments",
      "group": "function",
      "module": "llm.py",
      "submodule": "SPLIT FILES",
      "def": "def get_speaker_segments(file_path, skip_string='SKIPQA'):",
      "line": 294,
      "docstring": "Extract segments from a file that do not contain a specific skip string, or all segments if skip string is None.\n\n    :param file_path: string of the path to the file to be processed\n    :param skip_string: string of the substring used to identify segments to skip, or None to include all segments\n    :return: list of segments without the skip string, or all segments if skip string is None"
    },
    {
      "id": "llm.count_segment_tokens",
      "label": "count_segment_tokens",
      "group": "function",
      "module": "llm.py",
      "submodule": "SPLIT FILES",
      "def": "def count_segment_tokens(file_path, skip_string='SKIPQA'):",
      "line": 312,
      "docstring": "Count tokens in each segment of a file and provide token statistics.\n\n    :param file_path: string of the path to the file to be processed\n    :param skip_string: string of the substring used to identify segments to skip\n    :return: tuple containing (list of segments, list of token counts)"
    },
    {
      "id": "llm.plot_segment_tokens",
      "label": "plot_segment_tokens",
      "group": "function",
      "module": "llm.py",
      "submodule": "SPLIT FILES",
      "def": "def plot_segment_tokens(file_path):",
      "line": 331,
      "docstring": "Create a horizontal bar chart plot of token counts for each segment and save it as a PNG file.\n\n    :param file_path: string of the path to the file to be processed\n    :return: string of the path to the saved PNG file"
    },
    {
      "id": "llm.group_segments_select_speaker",
      "label": "group_segments_select_speaker",
      "group": "function",
      "module": "llm.py",
      "submodule": "SPLIT FILES",
      "def": "def group_segments_select_speaker(segments, speaker):",
      "line": 374,
      "docstring": "Groups consecutive segments not containing the specified speaker's name and selects segments where the speaker's name is found before the timestamp.\n    Calls get_timestamp from fileops.py to determine if the first line in a segment is a speaker line.\n\n    :param segments: list of text segments to be processed\n    :param speaker: string of the speaker's name to select segments\n    :return: list of text segments where the speaker's name is found before the timestamp"
    },
    {
      "id": "llm.group_segments_token_cap",
      "label": "group_segments_token_cap",
      "group": "function",
      "module": "llm.py",
      "submodule": "SPLIT FILES",
      "def": "def group_segments_token_cap(segments, token_cap=2000):",
      "line": 406,
      "docstring": "Groups consecutive segments without exceeding the token_cap, without splitting segments.\n    Includes segments that exceed the token_cap as individual blocks.\n\n    :param segments: list of text segments to be processed\n    :param token_cap: integer of maximum number of tokens, using words = .75 tokens\n    :return: list of grouped text segments without exceeding the token_cap, including oversized segments as individual blocks"
    },
    {
      "id": "llm.split_file_select_speaker",
      "label": "split_file_select_speaker",
      "group": "function",
      "module": "llm.py",
      "submodule": "SPLIT FILES",
      "def": "def split_file_select_speaker(file_path, speaker, skip_string='SKIPQA', suffix_new='_blocks'):",
      "line": 446,
      "docstring": "Add block delimiters to a file, with a block for every segment by the selected speaker and other segments grouped together.\n\n    :param file_path: path to the file to be processed\n    :param speaker: the speaker whose sections will be delimited\n    :param skip_string: string to identify speaker segments to skip\n    :param suffix_new: suffix for the new file with block delimiters\n    :return: file_path of new file with separator delimiters ('---') with suffix_new='_blocks' by default"
    },
    {
      "id": "llm.split_file_every_speaker",
      "label": "split_file_every_speaker",
      "group": "function",
      "module": "llm.py",
      "submodule": "SPLIT FILES",
      "def": "def split_file_every_speaker(file_path, skip_string=None, suffix_new='_blocks'):",
      "line": 466,
      "docstring": "Add block delimiters to a file with one block per speaker segment regardless of speaker.\n\n    :param file_path: path to the file to be processed\n    :param skip_string: string to identify speaker segments to skip\n    :param suffix_new: suffix for the new file with block delimiters\n    :return: file_path of new file with separator delimiters ('---') with suffix_new='_blocks' by default"
    },
    {
      "id": "llm.split_file_token_cap",
      "label": "split_file_token_cap",
      "group": "function",
      "module": "llm.py",
      "submodule": "SPLIT FILES",
      "def": "def split_file_token_cap(file_path, token_cap, skip_string='SKIPQA', suffix_new='_blocks'):",
      "line": 482,
      "docstring": "Add block delimiters to a file with one block per speaker segment regardless of speaker.\n\n    :param file_path: path to the file to be processed\n    :param token_cap: integer of maximum number of tokens, using words = .75 tokens\n    :param skip_string: string to identify speaker segments to skip\n    :param suffix_new: suffix for the new file with block delimiters\n    :return: file_path of new file with separator delimiters ('---') with suffix_new='_blocks' by default"
    },
    {
      "id": "llm.test_openai_chat",
      "label": "test_openai_chat",
      "group": "function",
      "module": "llm.py",
      "submodule": "OPENAI LLM",
      "def": "def test_openai_chat(model=OPENAI_MODEL):# DS, cat 5, unittests 2 APIMOCK",
      "line": 503,
      "docstring": "Sends a predefined message to the OpenAI chat API and prints the response.\n\n    :param model: string of the model name to be used for the chat completion request\n    :return: None"
    },
    {
      "id": "llm.openai_chat_completion_request",
      "label": "openai_chat_completion_request",
      "group": "function",
      "module": "llm.py",
      "submodule": "OPENAI LLM",
      "def": "def openai_chat_completion_request(messages, tools=None, tool_choice=None, model=OPENAI_MODEL):  # APIMOCK unittests 2",
      "line": 518,
      "docstring": "Send a chat completion request to the OpenAI API with the provided messages and optional tools and tool choice.\n\n    :param messages: a list of message dictionaries to send in the chat completion request\n    :param tools: optional list of tools to include in the request\n    :param tool_choice: optional tool choice to include in the request\n    :param model: the model to use for the chat completion request\n    :return: the response object from the OpenAI API request"
    },
    {
      "id": "llm.simple_openai_chat_completion_request",
      "label": "simple_openai_chat_completion_request",
      "group": "function",
      "module": "llm.py",
      "submodule": "OPENAI LLM",
      "def": "def simple_openai_chat_completion_request(prompt, model):  # no unittests",
      "line": 549,
      "docstring": "Sends a prompt and content to the OpenAI LLM and returns the assistant's message.\n\n    :param fcall_prompt: string of the system's prompt to initiate the conversation\n    :param content: string of the user's content to process\n    :param tools: list of dictionaries containing tool configurations\n    :param model: string specifying the OpenAI model to use\n    :param verbose: boolean indicating whether to print detailed response text\n    :return: string of the assistant's message from the LLM response"
    },
    {
      "id": "llm.openai_function_call",
      "label": "openai_function_call",
      "group": "function",
      "module": "llm.py",
      "submodule": "OPENAI LLM",
      "def": "def openai_function_call(fcall_prompt, content, tools, model=OPENAI_MODEL, verbose=False):  # APIMOCK unittests 3",
      "line": 574,
      "docstring": "Sends a prompt and content to the OpenAI LLM and returns the assistant's message.\n\n    :param fcall_prompt: string of the system's prompt to initiate the conversation\n    :param content: string of the user's content to process\n    :param tools: list of dictionaries containing tool configurations\n    :param model: string specifying the OpenAI model to use\n    :param verbose: boolean indicating whether to print detailed response text\n    :return: string of the assistant's message from the LLM response"
    },
    {
      "id": "llm.anthropic_chat_completion_request",
      "label": "anthropic_chat_completion_request",
      "group": "function",
      "module": "llm.py",
      "submodule": "ANTHROPIC LLM",
      "def": "def anthropic_chat_completion_request(messages, model=ANTHROPIC_MODEL, system=None, max_tokens=4096, temperature=0.7):",
      "line": 605,
      "docstring": "Make a chat completion request to Anthropic's API.\n\n    :param messages: List of message objects representing the conversation\n    :param model: The model to use for the completion\n    :param system: System message to set the behavior of the assistant\n    :param max_tokens: Maximum number of tokens to generate (default: 4096)\n    :param temperature: Controls randomness in the output (0 to 1, default: 0.7)\n    :return: The generated message content or None if an error occurs"
    },
    {
      "id": "llm.simple_anthropic_chat_completion_request",
      "label": "simple_anthropic_chat_completion_request",
      "group": "function",
      "module": "llm.py",
      "submodule": "ANTHROPIC LLM",
      "def": "def simple_anthropic_chat_completion_request(prompt, model=ANTHROPIC_MODEL):",
      "line": 650,
      "docstring": "Make a simple chat completion request to Anthropic's API.\n\n    :param prompt: String containing the user's prompt or message\n    :param model: String specifying the Anthropic model to use (default: 'claude-3-opus-20240229')\n    :return: String containing the generated message content, or an error message if the request fails"
    },
    {
      "id": "llm.llm_process_block",
      "label": "llm_process_block",
      "group": "function",
      "module": "llm.py",
      "submodule": "LLM PROCESSING",
      "def": "def llm_process_block(block, prompt, provider=\"openai\"):",
      "line": 687,
      "docstring": "Processes a single block of text with a given prompt using the OpenAI chat completion API.\n\n    :param block: string of the text block to be processed.\n    :param prompt: string of the prompt to use for the chat completion request.\n    :param provider: string indicating the LLM provider (default is 'openai').\n    :return: string of the processed text block or None if no valid response is received."
    },
    {
      "id": "llm.llm_process_file_blocks",
      "label": "llm_process_file_blocks",
      "group": "function",
      "module": "llm.py",
      "submodule": "LLM PROCESSING",
      "def": "def llm_process_file_blocks(blocks_file_path, prompt, suffix_new, mode, provider=\"openai\", retain_delimiters=False):",
      "line": 717,
      "docstring": "Processes blocks of text in a file using a specified prompt and operation mode, then writes the processed content back to the file.\n\n    :param blocks_file_path: string of the path to the file containing text blocks\n    :param prompt: string of the prompt to use for processing each text block\n    :param suffix_new: string of the suffix to append to the file when saving the new content\n    :param mode: string of the operation mode ('replace' or 'append') to handle the processed blocks\n    :param provider: string indicating the LLM provider (default is 'openai')\n    :param retain_delimiters: boolean indicating whether to retain the original block delimiters in the new content\n    :return: the path to the file with the updated content"
    },
    {
      "id": "llm.scall_replace",
      "label": "scall_replace",
      "group": "function",
      "module": "llm.py",
      "submodule": "LLM PROCESSING",
      "def": "def scall_replace(blocks_file_path, prompt, suffix_new='_scall-replace', provider=\"openai\", retain_delimiters=False):",
      "line": 762,
      "docstring": "Processes a file's text blocks and replace the original text with LLM-processed content based on a given prompt.\n    \n    :param blocks_file_path: string of the path to the file containing text blocks\n    :param prompt: string of the prompt to use for processing each text block\n    :param suffix_new: string of the suffix to append to the file when saving the new content\n    :param provider: string indicating the LLM provider (default is 'openai')\n    :param retain_delimiters: boolean indicating whether to retain the original block delimiters in the new content\n    :return: string of the path to the file with the updated content"
    },
    {
      "id": "llm.scall_append",
      "label": "scall_append",
      "group": "function",
      "module": "llm.py",
      "submodule": "LLM PROCESSING",
      "def": "def scall_append(blocks_file_path, prompt, suffix_new='_scall-append', provider=\"openai\", retain_delimiters=False):",
      "line": 775,
      "docstring": "Processes a file's text blocks to append LLM-processed content based on a given prompt after the original text.\n    \n    :param blocks_file_path: string of the path to the file containing text blocks\n    :param prompt: string of the prompt to use for processing each text block\n    :param suffix_new: string of the suffix to append to the file when saving the new content\n    :param provider: string indicating the LLM provider (default is 'openai')\n    :param retain_delimiters: boolean indicating whether to retain the original block delimiters in the new content\n    :return: string of the path to the file with the updated content"
    },
    {
      "id": "llm.create_simple_llm_file",
      "label": "create_simple_llm_file",
      "group": "function",
      "module": "llm.py",
      "submodule": "LLM PROCESSING",
      "def": "def create_simple_llm_file(file_path, prompt, suffix_new, mode, split_file_function, provider=\"openai\", *args, **kwargs):",
      "line": 788,
      "docstring": "Processes a file with a simple llm call to create a LLM-processed version using a specified block separation function and prompt.\n    Substitutes the suffix_new for the original suffix of the file_path.\n\n    :param file_path: string of the path to the file to be processed\n    :param prompt: string of the prompt to use for processing each text block\n    :param suffix_new: string of the new suffix that will be substituted for the original suffix\n    :param mode: string indicating the operation mode ('replace' or 'append')\n    :param split_file_function: function used to separate the file into blocks\n    :param provider: string indicating the LLM provider (default is 'openai')\n    :param args: additional positional arguments passed to the block separation function\n    :param kwargs: additional keyword arguments passed to the block separation function\n    :return: string of the path to the file with the updated content"
    },
    {
      "id": "llm.create_copyedit_file",
      "label": "create_copyedit_file",
      "group": "function",
      "module": "llm.py",
      "submodule": "COPYEDITS",
      "def": "def create_copyedit_file(file_path, split_file_function, prompt, *args, **kwargs):",
      "line": 952,
      "docstring": "Processes a file for copyediting by separating it into blocks, applying a prompt to each block, and appending the results to a new file with a '_copyedit' suffix.\n    Uses an argument to pass in the separator function, in case you want different types of blocks\n\n    :param file_path: string of the path to the file to be processed\n    :param split_file_function: function used to separate the file into blocks\n    :param prompt: string of the prompt to use for processing each text block\n    :param args: additional positional arguments passed to the block separation function\n    :param kwargs: additional keyword arguments passed to the block separation function\n    :return: string of the path to the file with the updated content"
    },
    {
      "id": "llm.mod_blocks_file_with_adjacent_words",
      "label": "mod_blocks_file_with_adjacent_words",
      "group": "function",
      "module": "llm.py",
      "submodule": "TRANSCRIPT TRANSITIONS",
      "def": "def mod_blocks_file_with_adjacent_words(blocks_file_path, num_adjacent_words):",
      "line": 1002,
      "docstring": "Modifies the content of a file by adding a specified number of words from the previous and next blocks to each block.\n    Also adds a Markdown heading and content at the beginning of the new content.\n    :param blocks_file_path: string of the path to the file containing text blocks\n    :param num_adjacent_words: integer indicating the number of words to add from adjacent blocks\n    :return: None"
    },
    {
      "id": "llm.scall_replace_adjacent_words",
      "label": "scall_replace_adjacent_words",
      "group": "function",
      "module": "llm.py",
      "submodule": "TRANSCRIPT TRANSITIONS",
      "def": "def scall_replace_adjacent_words(blocks_file_path, prompt, adjacent_words, retain_delimiters=False, suffix_new='_scall-replace-adj'):",
      "line": 1034,
      "docstring": "Replaces words adjacent to each block in a file with a language model processed version based on a given prompt.\n\n    :param blocks_file_path: string of the path to the file containing text blocks\n    :param prompt: string of the prompt to process each block with\n    :param adjacent_words: integer indicating the number of words to add from adjacent blocks\n    :param retain_delimiters: boolean indicating whether to retain original block delimiters\n    :param suffix_new: string of the suffix to append to the new file name\n    :return: string of the path to the modified file"
    },
    {
      "id": "llm.create_transitions_file",
      "label": "create_transitions_file",
      "group": "function",
      "module": "llm.py",
      "submodule": "TRANSCRIPT TRANSITIONS",
      "def": "def create_transitions_file(file_path, split_file_function, prompt, *args, **kwargs):",
      "line": 1048,
      "docstring": "Creates a file with transitions between blocks processed by a language model based on a given prompt.\n\n    :param file_path: string of the path to the original file\n    :param split_file_function: function used to separate the original file into blocks\n    :param prompt: string of the prompt to process each block with\n    :return: string of the path to the transitions file"
    },
    {
      "id": "llm.tools_qa_speaker",
      "label": "tools_qa_speaker",
      "group": "function",
      "module": "llm.py",
      "submodule": "QA GENERATION",
      "def": "def tools_qa_speaker(speaker):  # no unittests",
      "line": 1080,
      "docstring": "Generate a list of tools for question and answer extraction based on the speaker's response.\n\n    :param speaker: string of the speaker's name whose responses are being analyzed\n    :return: list of dictionaries containing tool configurations for QA extraction"
    },
    {
      "id": "llm.fcall_qa_speaker",
      "label": "fcall_qa_speaker",
      "group": "function",
      "module": "llm.py",
      "submodule": "QA GENERATION",
      "def": "def fcall_qa_speaker(block_file_path, speaker, fcall_prompt, suffix_new=\"_qa\"):  # skip unittests because called below",
      "line": 1114,
      "docstring": "Processes a transcript file already sectioned into blocks to generate new question and answer file.\n    Answers are based on the speaker segments of the provided speaker.\n    Uses OpenAI function calling.\n\n    :param block_file_path: string of the path to the _blocks file to be processed.\n    :param speaker: string of the speaker's name for the answers in QA.\n    :param fcall_prompt: string of the prompt to be used for function calling.\n    :param suffix_new: string of the suffix to be appended to the original filename for the new file. Defaults to '_qa'.\n    :return: string of the path to the newly created file with QA"
    },
    {
      "id": "llm.create_qa_file_select_speaker",
      "label": "create_qa_file_select_speaker",
      "group": "function",
      "module": "llm.py",
      "submodule": "QA GENERATION",
      "def": "def create_qa_file_select_speaker(file_path, speaker, fcall_prompt):",
      "line": 1168,
      "docstring": "Processes a _prepqa file to generate QA question and answer blocks using OpenAI LLM function calling.\n\n    :param file_path: string of the path to the _prepqa transcript file to be processed.\n    :param speaker: string of the speaker's name to be used in processing.\n    :return: None."
    },
    {
      "id": "llm.tools_qa_incremental",
      "label": "tools_qa_incremental",
      "group": "function",
      "module": "llm.py",
      "submodule": "QA GENERATION",
      "def": "def tools_qa_incremental():",
      "line": 1235,
      "docstring": "Get the next chunk of transcript to process, based on the algorithm specification.\n    \n    :param transcript: Complete transcript text.\n    :param start_position: Starting character position in the transcript.\n    :param next_tokens: Number of tokens to look ahead.\n    :return: Tuple of (chunk_text, end_position)."
    },
    {
      "id": "llm.get_next_chunk",
      "label": "get_next_chunk",
      "group": "function",
      "module": "llm.py",
      "submodule": "QA GENERATION",
      "def": "def get_next_chunk(transcript, start_position, next_tokens):",
      "line": 1291,
      "docstring": "Get the next chunk of transcript to process, based on the algorithm specification.\n    \n    :param transcript: Complete transcript text.\n    :param start_position: Starting character position in the transcript.\n    :param next_tokens: Number of tokens to look ahead.\n    :return: Tuple of (chunk_text, end_position)."
    },
    {
      "id": "llm.get_last_qa_block_start_position",
      "label": "get_last_qa_block_start_position",
      "group": "function",
      "module": "llm.py",
      "submodule": "QA GENERATION",
      "def": "def get_last_qa_block_start_position(qa_file_path):",
      "line": 1312,
      "docstring": "Read the last processed start position from the existing QA file.\n    \n    :param qa_file_path: String of the path to the QA file.\n    :return: Integer of the transcript start position, or 0 if not found."
    },
    {
      "id": "llm.fcall_qa_incremental",
      "label": "fcall_qa_incremental",
      "group": "function",
      "module": "llm.py",
      "submodule": "QA GENERATION",
      "def": "def fcall_qa_incremental(transcript, next_tokens, fcall_prompt, start_position):",
      "line": 1332,
      "docstring": "Processes a transcript string incrementally to extract the next question-answer pair using OpenAI function calling.\n    This function yields each QA block along with the current position in the transcript, allowing for incremental processing and resumption from the last processed position.  \n\n    :param transcript: String of the transcript content.\n    :param next_tokens: Integer of the number of tokens to look ahead.\n    :param fcall_prompt: String of the prompt to be used for function calling.\n    :param start_position: Integer of the starting position in the transcript.\n    :yield: Tuple of (qa_block, current_position) or (None, current_position) if an error occurred."
    },
    {
      "id": "llm.create_qa_file_from_transcript_incremental",
      "label": "create_qa_file_from_transcript_incremental",
      "group": "function",
      "module": "llm.py",
      "submodule": "QA GENERATION",
      "def": "def create_qa_file_from_transcript_incremental(file_path, fcall_prompt):",
      "line": 1387,
      "docstring": "Manages the incremental extraction of question-answer pairs from a transcript file.\n    This function handles the overall process, including reading the transcript, determining the next chunk to process, and appending the extracted QA blocks to a new file.\n\n    :param file_path: String of the path to the transcript file to be processed.\n    :param fcall_prompt: String of the prompt to be used for function calling.\n    :return: String of the path to the newly created QA file."
    },
    {
      "id": "llm.validate_qa_transcript_positions",
      "label": "validate_qa_transcript_positions",
      "group": "function",
      "module": "llm.py",
      "submodule": "QA EVAL",
      "def": "def validate_qa_transcript_positions(transcript, qa_dict):",
      "line": 1458,
      "docstring": "Validate the extracted QA block against the original transcript based on reported positions.\n    \n    :param transcript: String of the full transcript text.\n    :param qa_dict: Dictionary containing the extracted QA information.\n    :return: Tuple of (bool, str) indicating pass/fail and a mismatch description if applicable."
    },
    {
      "id": "llm.evaluate_qa_extraction",
      "label": "evaluate_qa_extraction",
      "group": "function",
      "module": "llm.py",
      "submodule": "QA EVAL",
      "def": "def evaluate_qa_extraction(transcript, qa_file_path):",
      "line": 1481,
      "docstring": "Evaluate the QA extraction process using LLM-based checks and position validation.\n    \n    :param transcript: String of the full transcript text.\n    :param qa_file_path: String path to the file containing extracted QA blocks.\n    :return: List of dictionaries containing evaluation results for each QA block."
    },
    {
      "id": "llm.generate_evaluation_report",
      "label": "generate_evaluation_report",
      "group": "function",
      "module": "llm.py",
      "submodule": "QA EVAL",
      "def": "def generate_evaluation_report(evaluation_results, output_file):",
      "line": 1567,
      "docstring": "Generate a readable report from the evaluation results.\n    \n    :param evaluation_results: List of dictionaries containing evaluation results.\n    :param output_file: String path to write the report."
    },
    {
      "id": "llm.run_automated_evaluation",
      "label": "run_automated_evaluation",
      "group": "function",
      "module": "llm.py",
      "submodule": "QA EVAL",
      "def": "def run_automated_evaluation(transcript_file, qa_file):",
      "line": 1601,
      "docstring": "Run the automated evaluation process.\n    \n    :param transcript_file: String path to the original transcript file.\n    :param qa_file: String path to the file containing extracted QA blocks."
    },
    {
      "id": "vectordb.generate_embedding",
      "label": "generate_embedding",
      "group": "function",
      "module": "vectordb.py",
      "submodule": "VECTOR DB SUPPORT",
      "def": "def generate_embedding(text, model=EMBEDDING_MODEL):",
      "line": 26,
      "docstring": "Generates an embedding vector for the provided text using the specified OpenAI embeddings model.\n\n    :param text: string of text to generate an embedding for.\n    :param model: string of the OpenAI embeddings model to use.\n    :return: list of floats representing the embedding vector."
    },
    {
      "id": "vectordb.generate_vectors_qa",
      "label": "generate_vectors_qa",
      "group": "function",
      "module": "vectordb.py",
      "submodule": "VECTOR DB SUPPORT",
      "def": "def generate_vectors_qa(folder_paths, suffixpat_include, include_subfolders=True):",
      "line": 41,
      "docstring": "Generates vectors from markdown files in the specified folder paths.\n\n    :param folder_paths: list of strings of the paths to the folders containing markdown files.\n    :param include_subfolders: boolean indicating whether to search for markdown files in subfolders. Default is True.\n    :return: vectors as a list of dictionaries, each containing an id, values, and metadata for a block of text."
    },
    {
      "id": "vectordb.vectors_to_json",
      "label": "vectors_to_json",
      "group": "function",
      "module": "vectordb.py",
      "submodule": "VECTOR DB SUPPORT",
      "def": "def vectors_to_json(vectors, file_path):",
      "line": 80,
      "docstring": "Converts a list of dictionaries into a JSON file.\n\n    :param vectors: list of dictionaries to be converted.\n    :param file_path: name of the JSON file to be created.\n    :return: None."
    },
    {
      "id": "vectordb.json_to_vectors",
      "label": "json_to_vectors",
      "group": "function",
      "module": "vectordb.py",
      "submodule": "VECTOR DB SUPPORT",
      "def": "def json_to_vectors(file_path):",
      "line": 98,
      "docstring": "Loads vectors from a JSON file.\n\n    :param file_path: Path to the JSON file containing the vectors.\n    :return: List of vectors loaded from the JSON file. Returns an empty list if an error occurs."
    },
    {
      "id": "vectordb.validate_vectors",
      "label": "validate_vectors",
      "group": "function",
      "module": "vectordb.py",
      "submodule": "VECTOR DB SUPPORT",
      "def": "def validate_vectors(vectors, required_fields=None, verbose=False):",
      "line": 114,
      "docstring": "Validates that each vector in the list has the required fields and correct data types.\n\n    :param vectors: list of dictionaries, each representing a vector with an id, values, and metadata.\n    :param required_fields: list of required field names (in uppercase). If None, uses a default set.\n    :param verbose: boolean, if True, prints additional information during validation.\n    :return: None. Raises ValueError if validation fails."
    },
    {
      "id": "vectordb.upsert_vectors_pinecone",
      "label": "upsert_vectors_pinecone",
      "group": "function",
      "module": "vectordb.py",
      "submodule": "VECTOR DB SUPPORT",
      "def": "def upsert_vectors_pinecone(vectors, vector_index_name, new_index=True):",
      "line": 143,
      "docstring": "Upserts vectors into a Pinecone index in batches of 100, creating the index if it does not exist and if new_index is True.\n\n    :param vectors: list of dictionaries, each representing a vector to be upserted.\n    :param vector_index_name: string of the name of the Pinecone index.\n    :param new_index: boolean indicating whether to create a new index if it does not exist. Default is True.\n    :return: None."
    },
    {
      "id": "vectordb.delete_pinecone_index",
      "label": "delete_pinecone_index",
      "group": "function",
      "module": "vectordb.py",
      "submodule": "VECTOR DB SUPPORT",
      "def": "def delete_pinecone_index(vector_index_name, user_prompt=True):",
      "line": 169,
      "docstring": "Deletes a Pinecone index if it exists, optionally prompting the user for confirmation.\n\n    :param vector_index_name: string of the name of the Pinecone index to be deleted.\n    :param user_prompt: boolean indicating whether to prompt the user for confirmation before deletion. Default is True.\n    :return: Boolean indicating whether the index was actually deleted."
    },
    {
      "id": "vectordb.update_pinecone_index_list_md",
      "label": "update_pinecone_index_list_md",
      "group": "function",
      "module": "vectordb.py",
      "submodule": "VECTOR DB SUPPORT",
      "def": "def update_pinecone_index_list_md(file_name='pinecone_index_list.md', log_folder_path=VZIP_LOG_FOLDER):",
      "line": 191,
      "docstring": "Updates a markdown file with a list of Pinecone indices.\n\n    :param file_name: Name of the markdown file to update. Default is 'pinecone_index_list.md'.\n    :param log_folder_path: Path to the folder where the file will be saved. Default is VZIP_LOG_FOLDER."
    },
    {
      "id": "vectordb.save_splits_to_json",
      "label": "save_splits_to_json",
      "group": "function",
      "module": "vectordb.py",
      "submodule": "VECTOR DB SUPPORT",
      "def": "def save_splits_to_json(all_chunks, output_base_filename, metadata, log_folder_path=VZIP_LOG_FOLDER):",
      "line": 216,
      "docstring": "Saves the text splits and metadata to a JSON file, organized by source.\n\n    :param all_chunks: List of Document objects containing the text splits.\n    :param output_base_filename: Base filename for the output JSON file.\n    :param metadata: Dictionary containing metadata to be included in the JSON.\n    :return: Path to the saved JSON file."
    },
    {
      "id": "vectordb.setup_create_vectordb",
      "label": "setup_create_vectordb",
      "group": "function",
      "module": "vectordb.py",
      "submodule": "VECTOR DB SUPPORT",
      "def": "def setup_create_vectordb(folder_paths, vector_index_base, suffixpat_include=None):",
      "line": 259,
      "docstring": "Sets up the initial parameters for creating a vector database.\n    \n    :param folder_paths: List of folder paths to process\n    :param vector_index_base: Base name for the vector index\n    :param suffixpat_include: Pattern to filter files (optional)\n    :return: Tuple of (vector_index_name, vector_index_name_with_timestamp, datetime, all_file_paths)"
    },
    {
      "id": "vectordb.check_and_create_pinecone_index",
      "label": "check_and_create_pinecone_index",
      "group": "function",
      "module": "vectordb.py",
      "submodule": "VECTOR DB SUPPORT",
      "def": "def check_and_create_pinecone_index(vector_index_name, dimension=1536, metric='cosine'):",
      "line": 293,
      "docstring": "Initializes Pinecone, checks if the specified index exists, creates it if it doesn't,\n    and prompts the user for action if the index already exists.\n    \n    :param vector_index_name: Name of the Pinecone index to check/create\n    :param dimension: Dimension of the vectors (default is 1536 for OpenAI embeddings)\n    :param metric: Distance metric to use (default is 'cosine')\n    :return: Boolean indicating whether to continue with the vector database creation"
    },
    {
      "id": "vectordb.log_zip_vectordb",
      "label": "log_zip_vectordb",
      "group": "function",
      "module": "vectordb.py",
      "submodule": "VECTOR DB SUPPORT",
      "def": "def log_zip_vectordb(vectors_file_path, vector_index_name_with_timestamp, metadata, file_paths_list, log_folder_path=VZIP_LOG_FOLDER):",
      "line": 332,
      "docstring": "Creates a log file for vector database creation and zips source files.\n\n    :param vectors_file_path: Path to the vectors file to be zipped.\n    :param vector_index_name_with_timestamp: Name of the vector index with timestamp.\n    :param metadata: Metadata dictionary containing relevant information.\n    :param file_paths_list: List of all file paths processed.\n    :param log_folder_path: Folder to store log files and zips.\n    :return: Tuple of (log_file_path, zip_file_path)"
    },
    {
      "id": "vectordb.create_vectordb_vrag_langchain",
      "label": "create_vectordb_vrag_langchain",
      "group": "function",
      "module": "vectordb.py",
      "submodule": "VECTOR DB CREATION",
      "def": "def create_vectordb_vrag_langchain(folder_paths, vector_index_base, suffixpat_include=None, skip_pinecone=False):  # pinecone index names can only contain - and not _",
      "line": 381,
      "docstring": "Establishes a Pinecone database using documents from the directory of markdown files. \n    If vector index name already exists in Pinecone, this aborts - so the index must be manually deleted in portal prior to running this.\n    VRAG is a different pipeline than QRAG, and currently does not support custom metadata. (RT 6-10-2024)\n    VRAG Is set up to mainly accept unstructured documents, and QRAG only accepts structured input documents.\n\n    :param folder_paths: list of strings of the paths leading to the Obsidian vaults.\n    :param vector_index_base: string of the base name for the Pinecone index, (may only contain -'s), to which the number of files and date are added. \n    :param suffixpat_include: string or None, specifying which file suffix patterns (_suffix and/or extension) to include.\n    :param skip_pinecone: boolean, whether to skip the pinecone index creation and upserting (for splitting and testing).\n    :return: None"
    },
    {
      "id": "vectordb.create_qrag_vectordb",
      "label": "create_qrag_vectordb",
      "group": "function",
      "module": "vectordb.py",
      "submodule": "VECTOR DB CREATION",
      "def": "def create_qrag_vectordb(folder_paths, vector_index_base, suffixpat_include=None):",
      "line": 475,
      "docstring": "Retrieves relevant question chunks from a Pinecone index based on the input question.\n\n    :param question: string of the input question to search for.\n    :param vector_index_name: string of the name of the Pinecone index to query.\n    :return: tuple containing fetched question chunks and a dictionary of retrieved IDs with their scores."
    },
    {
      "id": "rag.pinecone_retriever",
      "label": "pinecone_retriever",
      "group": "function",
      "module": "rag.py",
      "submodule": "RETRIEVAL",
      "def": "def pinecone_retriever(query, vector_index_name, num_chunks=5):",
      "line": 19,
      "docstring": "Retrieves relevant question chunks from a Pinecone index based on the input question.\n\n    :param question: string of the input question to search for.\n    :param vector_index_name: string of the name of the Pinecone index to query.\n    :return: tuple containing fetched question chunks and a dictionary of retrieved IDs with their scores."
    },
    {
      "id": "rag.print_vrag_display_text",
      "label": "print_vrag_display_text",
      "group": "function",
      "module": "rag.py",
      "submodule": "VRAG",
      "def": "def print_vrag_display_text(json_object, show_prompt=False):",
      "line": 50,
      "docstring": "Prints a formatted display text for VRAG (Vector Retrieval Augmented Generation) results.\n\n    :param json_object: dictionary containing VRAG results with 'content' key.\n    :param show_prompt: boolean to determine whether to show the full LLM prompt.\n    :return: None."
    },
    {
      "id": "rag.vrag_llm_call",
      "label": "vrag_llm_call",
      "group": "function",
      "module": "rag.py",
      "submodule": "VRAG",
      "def": "def vrag_llm_call(user_question, vector_index_name, vrag_preamble=VRAG_PREAMBLE_V1, llm_model=DEFAULT_LLM_MODEL, user_id='default', vrag_version=\"1.0\"):",
      "line": 74,
      "docstring": "Initiates a chat session using vector retrieval augmented generation (VRAG) with a specified question,\n    prompt template, and index name. Returns a JSON object with the results.\n\n    :param user_question: string of the question to initiate the chat with.\n    :param vector_index_name: string of the name of the pinecone index to use for retrieval.\n    :param vrag_preamble: string of the preamble used to format the chat prompt.\n    :param llm_model: string of the language model to use.\n    :param user_id: string of the user identifier.\n    :param bot_version: string of the bot version.\n    :return: dictionary containing the chat response and metadata."
    },
    {
      "id": "rag.select_chunks_qrag_1or2",
      "label": "select_chunks_qrag_1or2",
      "group": "function",
      "module": "rag.py",
      "submodule": "QRAG",
      "def": "def select_chunks_qrag_1or2(fetched_qa_chunks, retrieved_ids_scores):",
      "line": 119,
      "docstring": "Sorts and returns the most relevant chunks based on similarity score and 'STARS' rating.\n    Filters down to 1 or 2 chunks from the 5 fetched.\n\n    :param fetched_qchunks: dictionary of fetched question chunks from Pinecone.\n    :param retrieved_ids_scores: dictionary of retrieved IDs with their similarity scores.\n    :return: tuple containing the highest similarity chunk and the highest 'STARS' rated chunk (if different)."
    },
    {
      "id": "rag.parse_chunk_all",
      "label": "parse_chunk_all",
      "group": "function",
      "module": "rag.py",
      "submodule": "QRAG",
      "def": "def parse_chunk_all(chunk, simscores=None):",
      "line": 141,
      "docstring": "Formats information from a chunk and its optional similarity scores into a structured dictionary,\n    including all fields present in the chunk's metadata. Handles various data types including lists.\n\n    :param chunk: dictionary containing metadata and content of a document chunk.\n    :param simscores: optional dictionary of similarity scores keyed by chunk id.\n    :return: dictionary containing formatted chunk information."
    },
    {
      "id": "rag.safe_convert",
      "label": "safe_convert",
      "group": "function",
      "module": "rag.py",
      "submodule": "QRAG",
      "def": "def safe_convert(value):",
      "line": 150,
      "docstring": "Converts value to appropriate type, handling various data types including lists."
    },
    {
      "id": "rag.parse_chunk_qa_dd",
      "label": "parse_chunk_qa_dd",
      "group": "function",
      "module": "rag.py",
      "submodule": "QRAG",
      "def": "def parse_chunk_qa_dd(chunk, simscores, prefix=''):",
      "line": 177,
      "docstring": "Wrapper function that formats information from a chunk and its similarity scores into a structured dictionary,\n    maintaining the same functionality as the original parse_chunk_qa_dd function while using parse_chunk_all internally.\n    Handles complex data types like lists and empty strings.\n\n    :param chunk: dictionary containing metadata and content of a document chunk.\n    :param simscores: dictionary of similarity scores keyed by chunk id.\n    :param prefix: string of prefix to add to dictionary keys. default is empty string.\n    :return: dictionary containing formatted chunk information with prefixed keys."
    },
    {
      "id": "rag.safe_int",
      "label": "safe_int",
      "group": "function",
      "module": "rag.py",
      "submodule": "QRAG",
      "def": "def safe_int(value, default=0):",
      "line": 191,
      "docstring": "Routes a user question through a question retrieval augmented generation (QRAG) process.\n\n    :param user_question: string of the user's input question.\n    :param routes_dict: dictionary containing routing prompts and templates.\n    :param vector_index_name: string of the name of the pinecone index to use for retrieval.\n    :param routes_bounds: list of two floats representing the lower and upper similarity bounds for routing.\n    :param user_id: string of the user identifier. defaults to 'default'.\n    :param llm_model: string of the language model to use. defaults to DEFAULT_LLM_MODEL.\n    :param bot_version: string of the bot version. defaults to '1.0'.\n    :return: dictionary containing metadata and content of the QRAG process and response.\n\n    Usage:\n    response = qrag_routing_call('What is the capital of France?', routes_dict, 'my_index')"
    },
    {
      "id": "rag.safe_float",
      "label": "safe_float",
      "group": "function",
      "module": "rag.py",
      "submodule": "QRAG",
      "def": "def safe_float(value, default=0.0):",
      "line": 197,
      "docstring": "Routes a user question through a question retrieval augmented generation (QRAG) process.\n\n    :param user_question: string of the user's input question.\n    :param routes_dict: dictionary containing routing prompts and templates.\n    :param vector_index_name: string of the name of the pinecone index to use for retrieval.\n    :param routes_bounds: list of two floats representing the lower and upper similarity bounds for routing.\n    :param user_id: string of the user identifier. defaults to 'default'.\n    :param llm_model: string of the language model to use. defaults to DEFAULT_LLM_MODEL.\n    :param bot_version: string of the bot version. defaults to '1.0'.\n    :return: dictionary containing metadata and content of the QRAG process and response.\n\n    Usage:\n    response = qrag_routing_call('What is the capital of France?', routes_dict, 'my_index')"
    },
    {
      "id": "rag.qrag_routing_call",
      "label": "qrag_routing_call",
      "group": "function",
      "module": "rag.py",
      "submodule": "QRAG",
      "def": "def qrag_routing_call(user_question, vector_index_name, routes_dict, routes_bounds=[0.3, 0.9], llm_model=DEFAULT_LLM_MODEL, user_id='default', qrag_version=\"1.0\"):",
      "line": 220,
      "docstring": "Routes a user question through a question retrieval augmented generation (QRAG) process.\n\n    :param user_question: string of the user's input question.\n    :param routes_dict: dictionary containing routing prompts and templates.\n    :param vector_index_name: string of the name of the pinecone index to use for retrieval.\n    :param routes_bounds: list of two floats representing the lower and upper similarity bounds for routing.\n    :param user_id: string of the user identifier. defaults to 'default'.\n    :param llm_model: string of the language model to use. defaults to DEFAULT_LLM_MODEL.\n    :param bot_version: string of the bot version. defaults to '1.0'.\n    :return: dictionary containing metadata and content of the QRAG process and response.\n\n    Usage:\n    response = qrag_routing_call('What is the capital of France?', routes_dict, 'my_index')"
    },
    {
      "id": "rag.qrag_llm_call",
      "label": "qrag_llm_call",
      "group": "function",
      "module": "rag.py",
      "submodule": "QRAG",
      "def": "def qrag_llm_call(json_object):",
      "line": 311,
      "docstring": "Generates an AI answer for a given JSON object containing question and context information.\n\n    :param json_object: dictionary containing the question, context, and metadata for generating an AI answer.\n    :return: dictionary with the updated JSON object including the AI-generated answer."
    },
    {
      "id": "rag.print_qrag_display_text",
      "label": "print_qrag_display_text",
      "group": "function",
      "module": "rag.py",
      "submodule": "QRAG",
      "def": "def print_qrag_display_text(json_object):",
      "line": 341,
      "docstring": "Prints a formatted display text for QRAG (Question Retrieval Augmented Generation) results.\n\n    :param json_object: dictionary containing QRAG results with 'content' key.\n    :return: None."
    },
    {
      "id": "rag.qrag_2step",
      "label": "qrag_2step",
      "group": "function",
      "module": "rag.py",
      "submodule": "QRAG",
      "def": "def qrag_2step(user_question, routes_dict, vector_index_name):",
      "line": 355,
      "docstring": "Performs a two-step question-answering process using QRAG (Question Retrieval Augmented Generation).\n\n    :param user_question: string of the user's input question.\n    :return: None"
    },
    {
      "id": "conversion.convert_llamaparse_pdf_to_md",
      "label": "convert_llamaparse_pdf_to_md",
      "group": "function",
      "module": "conversion.py",
      "submodule": "LLAMAINDEX",
      "def": "def convert_llamaparse_pdf_to_md(file_path):",
      "line": 16,
      "docstring": "Converts Google Docs to Markdown using Llama Index.\n\n    :param gdoc_id_list: list of Google Docs document IDs.\n    :return: Markdown representation of the Google Docs."
    },
    {
      "id": "conversion.convert_llamaindex_gdocs_to_md",
      "label": "convert_llamaindex_gdocs_to_md",
      "group": "function",
      "module": "conversion.py",
      "submodule": "LLAMAINDEX",
      "def": "def convert_llamaindex_gdocs_to_md(gdoc_id_list):",
      "line": 27,
      "docstring": "Converts Google Docs to Markdown using Llama Index.\n\n    :param gdoc_id_list: list of Google Docs document IDs.\n    :return: Markdown representation of the Google Docs."
    },
    {
      "id": "conversion.convert_file_to_md_pandoc",
      "label": "convert_file_to_md_pandoc",
      "group": "function",
      "module": "conversion.py",
      "submodule": "PANDOC",
      "def": "def convert_file_to_md_pandoc(file_path, suffix_new=\"_pandoc\"):",
      "line": 78,
      "docstring": "Converts any pandoc supported file format to a markdown file using pypandoc.\n    Including but not limited to: doc, docx, html, latex, epub, odt, rtf, ascii doc.\n    pdf has limitations.\n\n    :param file_path: string of the path to the file to be converted."
    },
    {
      "id": "docwork.load_custom_dictionary",
      "label": "load_custom_dictionary",
      "group": "function",
      "module": "docwork.py",
      "submodule": "COMMON TEXT",
      "def": "def load_custom_dictionary(file_path):",
      "line": 7,
      "docstring": "# import nltk\n    # nltk.download('words')\n    # nltk.download('punkt')  # Added to download the 'punkt' tokenizer models\n    # from nltk.corpus import words\n#TODO rename and comment this function so it's more clear what it does"
    },
    {
      "id": "docwork.replace_colon_for_non_speaker",
      "label": "replace_colon_for_non_speaker",
      "group": "function",
      "module": "docwork.py",
      "submodule": "TRANSCRIPTS",
      "def": "def replace_colon_for_non_speaker(text):",
      "line": 25,
      "docstring": "Replaces colons that do not form part of speaker names with a space and a dash.\n\n    :param text: string of the transcript text that needs cleaning.\n    :return: string of the cleaned transcript text."
    },
    {
      "id": "docwork.reformat_transcript_text",
      "label": "reformat_transcript_text",
      "group": "function",
      "module": "docwork.py",
      "submodule": "TRANSCRIPTS",
      "def": "def reformat_transcript_text(original_text):",
      "line": 41,
      "docstring": "Reformats and cleans transcript text by separating speaker names from their dialogue.\n\n    This function performs the following operations:\n    1. Removes parentheses from the text.\n    2. Replaces colons in non-speaker contexts.\n    3. Identifies speaker names and separates them from their dialogue.\n    4. Formats the text so that each speaker name is on its own line, followed by their dialogue.\n    5. Removes extra spaces and ensures consistent formatting.\n\n    :param original_text: string of the raw transcript text to be cleaned and reformatted.\n    :return: string of the cleaned and restructured transcript text, with speaker names clearly separated from dialogue."
    },
    {
      "id": "docwork.flush_current_dialogue",
      "label": "flush_current_dialogue",
      "group": "function",
      "module": "docwork.py",
      "submodule": "TRANSCRIPTS",
      "def": "def flush_current_dialogue():",
      "line": 76,
      "docstring": "Helper function to flush the current dialogue to the fixed_lines."
    },
    {
      "id": "docwork.validate_transcript",
      "label": "validate_transcript",
      "group": "function",
      "module": "docwork.py",
      "submodule": "TRANSCRIPTS",
      "def": "def validate_transcript(file_path, verbose=False):",
      "line": 109,
      "docstring": "Validates the speaker segments in a single transcript file by checking the format of speaker segments.\n\n    :param file_path: string of path to the transcript file.\n    :param verbose: boolean indicating whether to print detailed response text\n    :return: boolean indicating whether the file passed the validation."
    },
    {
      "id": "docwork.extract_transcript_data",
      "label": "extract_transcript_data",
      "group": "function",
      "module": "docwork.py",
      "submodule": "TRANSCRIPTS",
      "def": "def extract_transcript_data(file_path):",
      "line": 170,
      "docstring": "Extracts detailed transcript information into a list of dictionaries.\n    Each dictionary contains speaker_name, speaker_role, timestamp, timestamp_link, and dialogue.\n    Speaker lines can end in a colon (FDA Townhalls) or include a timestamp (Deutsch, PV).\n    Speaker role is extracted from text within parentheses preceding the colon or timestamp.\n\n    :param file_path: string of the path to the file to be processed.\n    :return: list of dictionaries with keys ['speaker_name', 'speaker_role', 'timestamp', 'timestamp_link', 'dialogue']."
    },
    {
      "id": "docwork.create_speaker_triples",
      "label": "create_speaker_triples",
      "group": "function",
      "module": "docwork.py",
      "submodule": "TRANSCRIPTS",
      "def": "def create_speaker_triples(file_path):",
      "line": 235,
      "docstring": "Creates a list of speaker triples from a given file using extracted transcript data.\n    Works with speaker lines that end in a colon (FDA Townhalls) or have timestamp (Deutsch, PV).\n    Utilizes the extract_transcript_data function to parse the file and count occurrences of each speaker.\n    Then creates a list of triples in the format 'speaker name, file_stem, count'\n    file_stem is the filename without extension, and count is the number of times the speaker appears in the file.\n\n    :param file_path: string of the path to the file to be processed.\n    :return: string of speaker triples separated by newlines."
    },
    {
      "id": "docwork.create_speaker_matrix",
      "label": "create_speaker_matrix",
      "group": "function",
      "module": "docwork.py",
      "submodule": "TRANSCRIPTS",
      "def": "def create_speaker_matrix(folder_path, suffix_include=None, target_file_path=\"speaker_matrix.csv\"):",
      "line": 273,
      "docstring": "Creates a matrix of speaker names from files in a given folder and writes it to a CSV file.\n\n    This function processes all files in the specified folder that have the specified suffix, identifies the speakers \n    and counts their occurrences. It then creates a matrix of speaker triples in the format 'speaker, file_stem, count' \n    where speaker is the speaker's name, file_stem is the filename without extension, and count is the number of times \n    the speaker appears in the file. The matrix is written to a CSV file at the specified target file path.\n\n    :param folder_path: string of the path to the folder containing the files to be processed.\n    :param target_file_path: string of the path to the target CSV file. If no folder component is specified, the folder_path is used.\n    :param suffix_include: string of the suffix to include in the file processing.\n    :return: string of the path to the created csv file."
    },
    {
      "id": "docwork.extract_proper_names",
      "label": "extract_proper_names",
      "group": "function",
      "module": "docwork.py",
      "submodule": "PROPER NAMES",
      "def": "def extract_proper_names(text, custom_proper_names_files=None, bool_include_custom=False, verbose=False):",
      "line": 310,
      "docstring": "Extract proper names from the input text based on capitalization and dictionaries.\n    \n    :param text: string containing the input text.\n    :return: list of proper names identified."
    },
    {
      "id": "docwork.create_proper_names_triples",
      "label": "create_proper_names_triples",
      "group": "function",
      "module": "docwork.py",
      "submodule": "PROPER NAMES",
      "def": "def create_proper_names_triples(file_path, custom_proper_names_files=None, bool_include_custom=False, verbose=False):",
      "line": 388,
      "docstring": "Creates a list of proper name triples from a given file using extracted text data.\n    Utilizes the extract_proper_names function to parse the file and count occurrences of each proper name.\n    Then creates a list of triples in the format 'proper name, file_stem, count'\n    file_stem is the filename without extension, and count is the number of times the proper name appears in the file.\n\n    :param file_path: string of the path to the file to be processed.\n    :return: string of proper name triples separated by newlines."
    },
    {
      "id": "docwork.print_proper_names",
      "label": "print_proper_names",
      "group": "function",
      "module": "docwork.py",
      "submodule": "PROPER NAMES",
      "def": "def print_proper_names(file_path, custom_proper_names_files=None, bool_include_custom=False, verbose=False):",
      "line": 420,
      "docstring": "Extracts and validates blocks of text from a file.\n\n    :param qa_file_path: string of the path to the file to be read.\n    :param verbose: boolean, if True, prints verbose messages. Default is False.\n    :return: list of valid blocks from the file."
    },
    {
      "id": "structured.get_blocks_from_file",
      "label": "get_blocks_from_file",
      "group": "function",
      "module": "structured.py",
      "submodule": "BLOCK PROCESSING",
      "def": "def get_blocks_from_file(qa_file_path, verbose=False):",
      "line": 17,
      "docstring": "Extracts and validates blocks of text from a file.\n\n    :param qa_file_path: string of the path to the file to be read.\n    :param verbose: boolean, if True, prints verbose messages. Default is False.\n    :return: list of valid blocks from the file."
    },
    {
      "id": "structured.get_field_value",
      "label": "get_field_value",
      "group": "function",
      "module": "structured.py",
      "submodule": "BLOCK PROCESSING",
      "def": "def get_field_value(block, field):",
      "line": 46,
      "docstring": "Extracts the content of a specified field from a block of text.\n\n    :param block: string of the block of text to be processed.\n    :param field: string of the field to be extracted from the block.\n    :return: the content of the field in its appropriate data type, or None if the field is not found."
    },
    {
      "id": "structured.get_all_fields_dict",
      "label": "get_all_fields_dict",
      "group": "function",
      "module": "structured.py",
      "submodule": "BLOCK PROCESSING",
      "def": "def get_all_fields_dict(block):",
      "line": 74,
      "docstring": "Extracts all fields and their contents from a block of text.\n\n    :param block: string of the block of text to be processed.\n    :return: dictionary of fields and their contents in their appropriate data types."
    },
    {
      "id": "structured.count_blocks",
      "label": "count_blocks",
      "group": "function",
      "module": "structured.py",
      "submodule": "BLOCK PROCESSING",
      "def": "def count_blocks(file_path, heading=\"## content\"):  # quick way is to use find on a field",
      "line": 105,
      "docstring": "Counts the number of blocks in a specific section of a file, skipping comment lines.\n\n    :param file_path: string of the path to the file to be processed.\n    :param heading: string of the markdown heading to search for. Default is '## content'.\n    :return: integer representing the total number of blocks found."
    },
    {
      "id": "structured.extract_topic_counts_triples",
      "label": "extract_topic_counts_triples",
      "group": "function",
      "module": "structured.py",
      "submodule": "TOPICS",
      "def": "def extract_topic_counts_triples(qa_file_path, verbose=False):",
      "line": 133,
      "docstring": "Extracts topics from QA blocks in a file and counts their occurrences. \n\n    :param qa_file_path: string of the path to the QA file.\n    :param verbose: boolean, if True, prints additional information during execution. Default is False.\n    :return: string of CSV lines with each line in the format 'topic, file_stem, count'."
    },
    {
      "id": "structured.create_topics_matrix",
      "label": "create_topics_matrix",
      "group": "function",
      "module": "structured.py",
      "submodule": "TOPICS",
      "def": "def create_topics_matrix(folder_paths, target_file_path=\"matrix_topics.csv\", suffixpat_include=\"_qafixed\"):",
      "line": 167,
      "docstring": "Collects topics from files in specified folders and creates a CSV matrix file at the target file path.\n\n    :param folder_paths: list of strings of folder paths to search for files.\n    :param target_file_path: string of the path where the resulting CSV file will be created. If no folder is provided in the path, the parent folder of the first folder in the folder_paths list will be used.\n    :param suffix_include: string of the suffix to include in file search. Default is '_qafixed'.\n    :return: string of the path to the created csv file."
    },
    {
      "id": "structured.change_topic_in_file",
      "label": "change_topic_in_file",
      "group": "function",
      "module": "structured.py",
      "submodule": "TOPICS",
      "def": "def change_topic_in_file(file_path, find_topic, replace_topic):",
      "line": 198,
      "docstring": "Replaces a specified topic with another in a single file.\n\n    :param file_path: string of the file path to process.\n    :param find_topic: string of the topic to find.\n    :param replace_topic: string of the topic to use as a replacement.\n    :return: tuple of (int, int) representing (replacements_in_file, total_replacements)"
    },
    {
      "id": "structured.change_topic_in_folders",
      "label": "change_topic_in_folders",
      "group": "function",
      "module": "structured.py",
      "submodule": "TOPICS",
      "def": "def change_topic_in_folders(folder_paths, find_topic, replace_topic, suffixpat_include=\"_qafixed\"):",
      "line": 233,
      "docstring": "Replaces a specified topic with another across files in given folders.\n\n    :param folder_paths: list of strings of folder paths to search for files.\n    :param find_topic: string of the topic to find.\n    :param replace_topic: string of the topic to use as a replacement.\n    :param suffix_include: string of the suffix to include in file search.\n    :return: None."
    },
    {
      "id": "structured.process_file",
      "label": "process_file",
      "group": "function",
      "module": "structured.py",
      "submodule": "TOPICS",
      "def": "def process_file(file_path):",
      "line": 248,
      "docstring": "Validates the structure and content of QA blocks against required fields.\n\n    :param blocks_list: List of QA blocks, each block is a string of text representing a QA entry.\n    :param required_fields: List of required field names.\n    :param custom_validators: Dictionary of field names and their corresponding validation functions (default: None).\n    :return: Integer representing the number of valid blocks if all are valid, or the negative count of invalid blocks."
    },
    {
      "id": "structured.review_singlet_topic_SONNET",
      "label": "review_singlet_topic_SONNET",
      "group": "function",
      "module": "structured.py",
      "submodule": "TOPICS",
      "def": "def review_singlet_topic_SONNET(folder_paths, matrix_csv_file_path, starting_letter=\"a\"):",
      "line": 264,
      "docstring": "Validates the structure and content of QA blocks against required fields.\n\n    :param blocks_list: List of QA blocks, each block is a string of text representing a QA entry.\n    :param required_fields: List of required field names.\n    :param custom_validators: Dictionary of field names and their corresponding validation functions (default: None).\n    :return: Integer representing the number of valid blocks if all are valid, or the negative count of invalid blocks."
    },
    {
      "id": "structured.review_singlet_topic",
      "label": "review_singlet_topic",
      "group": "function",
      "module": "structured.py",
      "submodule": "TOPICS",
      "def": "def review_singlet_topic(folder_paths, matrix_csv_file_path, starting_letter=\"a\"):",
      "line": 315,
      "docstring": "Validates the structure and content of QA blocks against required fields.\n\n    :param blocks_list: List of QA blocks, each block is a string of text representing a QA entry.\n    :param required_fields: List of required field names.\n    :param custom_validators: Dictionary of field names and their corresponding validation functions (default: None).\n    :return: Integer representing the number of valid blocks if all are valid, or the negative count of invalid blocks."
    },
    {
      "id": "corpuses.validate_stars",
      "label": "validate_stars",
      "group": "function",
      "module": "corpuses.py",
      "submodule": "QA VALIDATION",
      "def": "def validate_stars(stars_str):",
      "line": 13,
      "docstring": "Validates the structure and content of QA blocks against required fields.\n\n    :param blocks_list: List of QA blocks, each block is a string of text representing a QA entry.\n    :param required_fields: List of required field names.\n    :param custom_validators: Dictionary of field names and their corresponding validation functions (default: None).\n    :return: Integer representing the number of valid blocks if all are valid, or the negative count of invalid blocks."
    },
    {
      "id": "corpuses.validate_topics",
      "label": "validate_topics",
      "group": "function",
      "module": "corpuses.py",
      "submodule": "QA VALIDATION",
      "def": "def validate_topics(topics_str):",
      "line": 22,
      "docstring": "Validates the structure and content of QA blocks against required fields.\n\n    :param blocks_list: List of QA blocks, each block is a string of text representing a QA entry.\n    :param required_fields: List of required field names.\n    :param custom_validators: Dictionary of field names and their corresponding validation functions (default: None).\n    :return: Integer representing the number of valid blocks if all are valid, or the negative count of invalid blocks."
    },
    {
      "id": "corpuses.validate_qa_blocks",
      "label": "validate_qa_blocks",
      "group": "function",
      "module": "corpuses.py",
      "submodule": "QA VALIDATION",
      "def": "def validate_qa_blocks(blocks_list, required_fields, custom_validators=None):",
      "line": 28,
      "docstring": "Validates the structure and content of QA blocks against required fields.\n\n    :param blocks_list: List of QA blocks, each block is a string of text representing a QA entry.\n    :param required_fields: List of required field names.\n    :param custom_validators: Dictionary of field names and their corresponding validation functions (default: None).\n    :return: Integer representing the number of valid blocks if all are valid, or the negative count of invalid blocks."
    },
    {
      "id": "corpuses.is_valid_file_qa",
      "label": "is_valid_file_qa",
      "group": "function",
      "module": "corpuses.py",
      "submodule": "QA VALIDATION",
      "def": "def is_valid_file_qa(file_path, required_fields, custom_validators, verbose=False):",
      "line": 81,
      "docstring": "Function to validate QA blocks in a file and return True if all blocks are valid\n\n    :param file_path: string of the path to the file to be validated\n    :param required_fields: List of required field names.\n    :param custom_validators: Dictionary of field names and their corresponding validation functions.\n    :param verbose: boolean to control verbose output\n    :return: boolean indicating whether all blocks in the file are valid"
    },
    {
      "id": "corpuses.validate_folders_qa",
      "label": "validate_folders_qa",
      "group": "function",
      "module": "corpuses.py",
      "submodule": "QA VALIDATION",
      "def": "def validate_folders_qa(folder_paths, required_fields, custom_validators, suffixpat_include=\"_qafixed\"):",
      "line": 101,
      "docstring": "Validates QA blocks in all files within specified folders, printing the number of valid files in each folder\n    and statistics about required and optional fields.\n\n    :param folder_paths: list of strings of folder paths to search for files.\n    :param required_fields: List of required field names.\n    :param custom_validators: Dictionary of field names and their corresponding validation functions.\n    :param suffixpat_include: string of the suffix to include in file search. Default is '_qafixed'.\n    :return: string of the path of the first file with invalid QA blocks if any; None if all files are valid."
    },
    {
      "id": "corpuses.validate_corpus_deutsch",
      "label": "validate_corpus_deutsch",
      "group": "function",
      "module": "corpuses.py",
      "submodule": "DEUTSCH",
      "def": "def validate_corpus_deutsch():",
      "line": 156,
      "docstring": "Validates the structure and content of QA blocks against required and optional fields.\n\n    :param blocks_list: list of qa blocks where each block is a string of text representing a qa entry.\n    :return: the number of valid blocks if all are valid, or the negative count of invalid blocks."
    },
    {
      "id": "corpuses.validate_qa_blocks_deutsch_OLD",
      "label": "validate_qa_blocks_deutsch_OLD",
      "group": "function",
      "module": "corpuses.py",
      "submodule": "DEUTSCH",
      "def": "def validate_qa_blocks_deutsch_OLD(blocks_list):",
      "line": 173,
      "docstring": "Validates the structure and content of QA blocks against required and optional fields.\n\n    :param blocks_list: list of qa blocks where each block is a string of text representing a qa entry.\n    :return: the number of valid blocks if all are valid, or the negative count of invalid blocks."
    },
    {
      "id": "corpuses.remove_lines_fda_townhall",
      "label": "remove_lines_fda_townhall",
      "group": "function",
      "module": "corpuses.py",
      "submodule": "FDA TOWNHALLS",
      "def": "def remove_lines_fda_townhall(text):",
      "line": 256,
      "docstring": "Remove lines from a string of FDA townhall transcript text that match certain patterns.\n\n    :param text: string of the transcript text to be cleaned.\n    :return: string of the cleaned transcript text."
    },
    {
      "id": "corpuses.clean_fda_townhall",
      "label": "clean_fda_townhall",
      "group": "function",
      "module": "corpuses.py",
      "submodule": "FDA TOWNHALLS",
      "def": "def clean_fda_townhall(file_path):",
      "line": 302,
      "docstring": "Cleans the FDA townhall file by removing unnecessary lines and fixing speaker text.\n\n    :param file_path: string of the path to the file to be cleaned.\n    :param suffix_new: string of the suffix to be added to the cleaned file. Default is '_cleaned'.\n    :return: The cleaned text with the heading set."
    },
    {
      "id": "corpuses.run_clean_on_fda_townhalls",
      "label": "run_clean_on_fda_townhalls",
      "group": "function",
      "module": "corpuses.py",
      "submodule": "FDA TOWNHALLS",
      "def": "def run_clean_on_fda_townhalls(source_folder, destination_folder):",
      "line": 322,
      "docstring": "Cleans the files in the source folder and moves the cleaned files to the destination folder.\n\n    :param source_folder: string of the path to the source folder containing the files to be cleaned.\n    :param destination_folder: string of the path to the destination folder where the cleaned files will be moved.\n    :return: None"
    },
    {
      "id": "corpuses.run_fix_names_on_fda_townhalls",
      "label": "run_fix_names_on_fda_townhalls",
      "group": "function",
      "module": "corpuses.py",
      "submodule": "FDA TOWNHALLS",
      "def": "def run_fix_names_on_fda_townhalls():  # COPY to your run file and use there",
      "line": 336,
      "docstring": "Validates the structure and content of QA blocks against required and optional fields.\n\n    :param blocks_list: list of qa blocks where each block is a string of text representing a qa entry.\n    :return: the number of valid blocks if all are valid, or the negative count of invalid blocks."
    },
    {
      "id": "corpuses.validate_qa_blocks_townhall_OLD",
      "label": "validate_qa_blocks_townhall_OLD",
      "group": "function",
      "module": "corpuses.py",
      "submodule": "FDA TOWNHALLS",
      "def": "def validate_qa_blocks_townhall_OLD(blocks_list):",
      "line": 356,
      "docstring": "Validates the structure and content of QA blocks against required and optional fields.\n\n    :param blocks_list: list of qa blocks where each block is a string of text representing a qa entry.\n    :return: the number of valid blocks if all are valid, or the negative count of invalid blocks."
    },
    {
      "id": "corpuses.validate_qa_blocks_townhall",
      "label": "validate_qa_blocks_townhall",
      "group": "function",
      "module": "corpuses.py",
      "submodule": "FDA TOWNHALLS",
      "def": "def validate_qa_blocks_townhall(blocks_list):",
      "line": 438,
      "docstring": "Upload a file to an S3 bucket\n\n    :param file_path: File to upload\n    :param bucket: Name of the S3 bucket, default is 'fofpublic', others: 'fofsecure', 'deutsch-audio'\n    :param object_name: S3 object name. If not specified, the file name is used\n    :param s3_path: S3 folder path where the file will be stored, e.g. 'podcasts/'\n    :return: The object name of the file in S3"
    },
    {
      "id": "aws.upload_file_to_s3",
      "label": "upload_file_to_s3",
      "group": "function",
      "module": "aws.py",
      "submodule": "AWS S3",
      "def": "def upload_file_to_s3(file_path, bucket='fofpublic', object_name=None, s3_path=None):",
      "line": 7,
      "docstring": "Upload a file to an S3 bucket\n\n    :param file_path: File to upload\n    :param bucket: Name of the S3 bucket, default is 'fofpublic', others: 'fofsecure', 'deutsch-audio'\n    :param object_name: S3 object name. If not specified, the file name is used\n    :param s3_path: S3 folder path where the file will be stored, e.g. 'podcasts/'\n    :return: The object name of the file in S3"
    },
    {
      "id": "aws.rename_s3_object",
      "label": "rename_s3_object",
      "group": "function",
      "module": "aws.py",
      "submodule": "AWS S3",
      "def": "def rename_s3_object(bucket, old_key, new_key, s3_path=None):",
      "line": 40,
      "docstring": "Rename an object in an S3 bucket by copying it to a new key and deleting the old key.\n\n    :param bucket: Name of the S3 bucket\n    :param old_key: The current key (path) of the object in the S3 bucket\n    :param new_key: The new key (path) for the object in the S3 bucket\n    :param s3_path: Optional S3 folder path to prepend to the keys\n    :return: None"
    },
    {
      "id": "aws.get_s3_json",
      "label": "get_s3_json",
      "group": "function",
      "module": "aws.py",
      "submodule": "AWS S3",
      "def": "def get_s3_json(bucket, key, s3_path=None):",
      "line": 67,
      "docstring": "Retrieve a JSON object from an S3 bucket.\n\n    :param bucket: Name of the S3 bucket\n    :param key: The key (path) of the JSON object in the S3 bucket\n    :param s3_path: Optional S3 folder path to prepend to the key\n    :return: The JSON object if found, otherwise None"
    }
  ],
  "edges": [
    {
      "from": "fileops.sub_suffix_in_str",
      "to": "fileops.get_suffix"
    },
    {
      "from": "fileops.remove_all_suffixes_in_str",
      "to": "fileops.get_suffix"
    },
    {
      "from": "fileops.sub_suffix_in_file",
      "to": "fileops.sub_suffix_in_str"
    },
    {
      "from": "fileops.count_suffixes_in_folder",
      "to": "fileops.get_suffix"
    },
    {
      "from": "fileops.get_files_in_folder",
      "to": "fileops.get_suffix"
    },
    {
      "from": "fileops.apply_to_folder",
      "to": "fileops.get_files_in_folder"
    },
    {
      "from": "fileops.apply_to_folder",
      "to": "fileops.verbose_print"
    },
    {
      "from": "fileops.read_file_flex",
      "to": "fileops.read_complete_text"
    },
    {
      "from": "fileops.read_file_flex",
      "to": "fileops.read_metadata_and_content"
    },
    {
      "from": "fileops.handle_overwrite_prompt",
      "to": "fileops.get_suffix"
    },
    {
      "from": "fileops.handle_overwrite_prompt",
      "to": "fileops.sub_suffix_in_str"
    },
    {
      "from": "fileops.handle_overwrite_prompt",
      "to": "fileops.verbose_print"
    },
    {
      "from": "fileops.manage_file_overwrite",
      "to": "fileops.add_suffix_in_str"
    },
    {
      "from": "fileops.manage_file_overwrite",
      "to": "fileops.handle_overwrite_prompt"
    },
    {
      "from": "fileops.manage_file_overwrite",
      "to": "fileops.sub_suffix_in_str"
    },
    {
      "from": "fileops.manage_file_overwrite",
      "to": "fileops.verbose_print"
    },
    {
      "from": "fileops.write_complete_text",
      "to": "fileops.add_suffix_in_str"
    },
    {
      "from": "fileops.write_complete_text",
      "to": "fileops.manage_file_overwrite"
    },
    {
      "from": "fileops.write_complete_text",
      "to": "fileops.warn_file_overwrite"
    },
    {
      "from": "fileops.write_metadata_and_content",
      "to": "fileops.write_complete_text"
    },
    {
      "from": "fileops.pretty_print_json_structure",
      "to": "fileops.print_json_structure"
    },
    {
      "from": "fileops.delete_files_with_suffix",
      "to": "fileops.apply_to_folder"
    },
    {
      "from": "fileops.apply_to_folder",
      "to": "fileops.delete_file",
      "type": "controller_to_worker",
      "applies_to_functions": [
        "fileops.delete_files_with_suffix"
      ]
    },
    {
      "from": "fileops.move_files_with_suffix",
      "to": "fileops.apply_to_folder"
    },
    {
      "from": "fileops.apply_to_folder",
      "to": "fileops.move_file",
      "type": "controller_to_worker",
      "applies_to_functions": [
        "fileops.move_files_with_suffix"
      ]
    },
    {
      "from": "fileops.create_full_path",
      "to": "fileops.get_suffix"
    },
    {
      "from": "fileops.create_full_path",
      "to": "fileops.tune_title"
    },
    {
      "from": "fileops.compare_files_text",
      "to": "fileops.read_complete_text"
    },
    {
      "from": "fileops.check_if_duplicate_filename",
      "to": "fileops.get_suffix"
    },
    {
      "from": "fileops.change_timestamp",
      "to": "fileops.convert_seconds_to_timestamp"
    },
    {
      "from": "fileops.change_timestamp",
      "to": "fileops.convert_timestamp_to_seconds"
    },
    {
      "from": "fileops.tune_timestamp",
      "to": "fileops.convert_seconds_to_timestamp"
    },
    {
      "from": "fileops.tune_timestamp",
      "to": "fileops.convert_timestamp_to_seconds"
    },
    {
      "from": "fileops.get_timestamp",
      "to": "fileops.convert_timestamp_to_seconds"
    },
    {
      "from": "fileops.get_current_datetime_filefriendly",
      "to": "fileops.get_current_datetime_humanfriendly"
    },
    {
      "from": "fileops.convert_to_epoch_seconds",
      "to": "fileops.verbose_print"
    },
    {
      "from": "fileops.remove_timestamp_links",
      "to": "fileops.read_file_flex"
    },
    {
      "from": "fileops.remove_timestamp_links",
      "to": "fileops.remove_timestamp_links_from_content"
    },
    {
      "from": "fileops.remove_timestamp_links",
      "to": "fileops.write_complete_text"
    },
    {
      "from": "fileops.remove_timestamp_links",
      "to": "fileops.write_metadata_and_content"
    },
    {
      "from": "fileops.generate_timestamp_link",
      "to": "fileops.convert_timestamp_to_seconds"
    },
    {
      "from": "fileops.add_timestamp_links_to_content",
      "to": "fileops.generate_timestamp_link"
    },
    {
      "from": "fileops.add_timestamp_links_to_content",
      "to": "fileops.get_timestamp"
    },
    {
      "from": "fileops.add_timestamp_links_to_content",
      "to": "fileops.tune_timestamp"
    },
    {
      "from": "fileops.add_timestamp_links",
      "to": "fileops.add_timestamp_links_to_content"
    },
    {
      "from": "fileops.add_timestamp_links",
      "to": "fileops.read_metadata_and_content"
    },
    {
      "from": "fileops.add_timestamp_links",
      "to": "fileops.read_metadata_field_from_file"
    },
    {
      "from": "fileops.add_timestamp_links",
      "to": "fileops.remove_timestamp_links_from_content"
    },
    {
      "from": "fileops.add_timestamp_links",
      "to": "fileops.write_metadata_and_content"
    },
    {
      "from": "fileops.count_num_instances",
      "to": "fileops.read_complete_text"
    },
    {
      "from": "fileops.find_and_replace_pairs",
      "to": "fileops.read_file_flex"
    },
    {
      "from": "fileops.find_and_replace_pairs",
      "to": "fileops.write_complete_text"
    },
    {
      "from": "fileops.find_and_replace_pairs",
      "to": "fileops.write_metadata_and_content"
    },
    {
      "from": "fileops.find_and_replace_from_csv",
      "to": "fileops.apply_to_folder"
    },
    {
      "from": "fileops.apply_to_folder",
      "to": "fileops.find_and_replace_pairs",
      "type": "controller_to_worker",
      "applies_to_functions": [
        "fileops.find_and_replace_from_csv"
      ]
    },
    {
      "from": "fileops.find_and_replace_from_csv",
      "to": "fileops.parse_csv_for_find_replace"
    },
    {
      "from": "fileops.get_heading_pattern",
      "to": "fileops.get_heading_level"
    },
    {
      "from": "fileops.find_heading_text",
      "to": "fileops.get_heading_pattern"
    },
    {
      "from": "fileops.get_heading",
      "to": "fileops.find_heading_text"
    },
    {
      "from": "fileops.get_heading",
      "to": "fileops.read_complete_text"
    },
    {
      "from": "fileops.set_heading",
      "to": "fileops.find_heading_text"
    },
    {
      "from": "fileops.set_heading",
      "to": "fileops.read_metadata_and_content"
    },
    {
      "from": "fileops.set_heading",
      "to": "fileops.write_metadata_and_content"
    },
    {
      "from": "fileops.delete_heading",
      "to": "fileops.find_heading_text"
    },
    {
      "from": "fileops.delete_heading",
      "to": "fileops.read_metadata_and_content"
    },
    {
      "from": "fileops.delete_heading",
      "to": "fileops.write_metadata_and_content"
    },
    {
      "from": "fileops.append_heading_to_file",
      "to": "fileops.get_heading"
    },
    {
      "from": "fileops.append_heading_to_file",
      "to": "fileops.read_complete_text"
    },
    {
      "from": "fileops.append_heading_to_file",
      "to": "fileops.write_complete_text"
    },
    {
      "from": "fileops.create_new_file_from_heading",
      "to": "fileops.get_heading"
    },
    {
      "from": "fileops.create_new_file_from_heading",
      "to": "fileops.write_complete_text"
    },
    {
      "from": "fileops.set_last_updated",
      "to": "fileops.set_metadata_field"
    },
    {
      "from": "fileops.read_metadata_field_from_file",
      "to": "fileops.read_metadata_and_content"
    },
    {
      "from": "fileops.set_metadata_fields_from_csv",
      "to": "fileops.read_metadata_and_content"
    },
    {
      "from": "fileops.set_metadata_fields_from_csv",
      "to": "fileops.set_metadata_field"
    },
    {
      "from": "fileops.set_metadata_fields_from_csv",
      "to": "fileops.write_metadata_and_content"
    },
    {
      "from": "fileops.create_csv_from_fields",
      "to": "fileops.get_heading"
    },
    {
      "from": "fileops.print_json_structure",
      "to": "fileops.print_json_structure"
    },
    {
      "from": "transcribe.get_youtube_title_length",
      "to": "fileops.tune_timestamp"
    },
    {
      "from": "transcribe.download_link_list_to_mp3s",
      "to": "transcribe.download_mp3_from_youtube"
    },
    {
      "from": "transcribe.download_link_list_to_mp3s",
      "to": "transcribe.get_youtube_title_length"
    },
    {
      "from": "transcribe.get_youtube_subtitles",
      "to": "transcribe.download_youtube_subtitles_url"
    },
    {
      "from": "transcribe.get_youtube_all",
      "to": "transcribe.download_youtube_subtitles_url"
    },
    {
      "from": "transcribe.get_youtube_all",
      "to": "transcribe.is_valid_youtube_url"
    },
    {
      "from": "transcribe.create_youtube_md",
      "to": "fileops.add_timestamp_links"
    },
    {
      "from": "transcribe.create_youtube_md",
      "to": "fileops.create_full_path"
    },
    {
      "from": "transcribe.create_youtube_md",
      "to": "fileops.set_metadata_field"
    },
    {
      "from": "transcribe.create_youtube_md",
      "to": "fileops.write_metadata_and_content"
    },
    {
      "from": "transcribe.create_youtube_md",
      "to": "transcribe.get_youtube_all"
    },
    {
      "from": "transcribe.create_youtube_md",
      "to": "transcribe.get_youtube_title_length"
    },
    {
      "from": "transcribe.create_youtube_md",
      "to": "transcribe.is_valid_youtube_url"
    },
    {
      "from": "transcribe.create_youtube_md_from_file_link",
      "to": "fileops.read_metadata_and_content"
    },
    {
      "from": "transcribe.create_youtube_md_from_file_link",
      "to": "fileops.read_metadata_field_from_file"
    },
    {
      "from": "transcribe.create_youtube_md_from_file_link",
      "to": "fileops.sub_suffix_in_str"
    },
    {
      "from": "transcribe.create_youtube_md_from_file_link",
      "to": "transcribe.create_youtube_md"
    },
    {
      "from": "transcribe.get_media_length",
      "to": "fileops.tune_timestamp"
    },
    {
      "from": "transcribe.get_media_length",
      "to": "transcribe.get_youtube_title_length"
    },
    {
      "from": "transcribe.get_media_length",
      "to": "transcribe.is_valid_youtube_url"
    },
    {
      "from": "transcribe.transcribe_deepgram",
      "to": "fileops.convert_seconds_to_timestamp"
    },
    {
      "from": "transcribe.transcribe_deepgram",
      "to": "fileops.convert_timestamp_to_seconds"
    },
    {
      "from": "transcribe.transcribe_deepgram",
      "to": "fileops.convert_to_epoch_seconds"
    },
    {
      "from": "transcribe.transcribe_deepgram",
      "to": "fileops.get_current_datetime_humanfriendly"
    },
    {
      "from": "transcribe.transcribe_deepgram",
      "to": "fileops.get_elapsed_seconds"
    },
    {
      "from": "transcribe.transcribe_deepgram",
      "to": "transcribe.get_media_length"
    },
    {
      "from": "transcribe.transcribe_deepgram_sdk_prerecorded",
      "to": "fileops.convert_seconds_to_timestamp"
    },
    {
      "from": "transcribe.transcribe_deepgram_sdk_prerecorded",
      "to": "fileops.convert_timestamp_to_seconds"
    },
    {
      "from": "transcribe.transcribe_deepgram_sdk_prerecorded",
      "to": "fileops.convert_to_epoch_seconds"
    },
    {
      "from": "transcribe.transcribe_deepgram_sdk_prerecorded",
      "to": "fileops.get_current_datetime_humanfriendly"
    },
    {
      "from": "transcribe.transcribe_deepgram_sdk_prerecorded",
      "to": "fileops.get_elapsed_seconds"
    },
    {
      "from": "transcribe.transcribe_deepgram_sdk_prerecorded",
      "to": "transcribe.get_media_length"
    },
    {
      "from": "transcribe.transcribe_deepgram_callback",
      "to": "fileops.get_current_datetime_humanfriendly"
    },
    {
      "from": "transcribe.transcribe_deepgram_callback",
      "to": "transcribe.get_media_length"
    },
    {
      "from": "transcribe.transcribe_deepgram_callback2",
      "to": "fileops.get_current_datetime_humanfriendly"
    },
    {
      "from": "transcribe.transcribe_deepgram_callback2",
      "to": "transcribe.get_media_length"
    },
    {
      "from": "transcribe.format_feature_segment",
      "to": "fileops.convert_seconds_to_timestamp"
    },
    {
      "from": "transcribe.format_feature_segment",
      "to": "transcribe.get_summary_start_seconds"
    },
    {
      "from": "transcribe.extract_feature_from_deepgram_json",
      "to": "transcribe.format_feature_segment"
    },
    {
      "from": "transcribe.set_various_transcript_headings",
      "to": "fileops.add_suffix_in_str"
    },
    {
      "from": "transcribe.set_various_transcript_headings",
      "to": "fileops.find_file_in_folders"
    },
    {
      "from": "transcribe.set_various_transcript_headings",
      "to": "fileops.remove_all_suffixes_in_str"
    },
    {
      "from": "transcribe.set_various_transcript_headings",
      "to": "fileops.set_heading"
    },
    {
      "from": "transcribe.set_various_transcript_headings",
      "to": "transcribe.extract_feature_from_deepgram_json"
    },
    {
      "from": "transcribe.set_various_transcript_headings",
      "to": "transcribe.extract_feature_from_youtube_md"
    },
    {
      "from": "transcribe.print_num_exception",
      "to": "transcribe.extract_context"
    },
    {
      "from": "transcribe.convert_num_line_lowercase",
      "to": "transcribe.get_previous_word"
    },
    {
      "from": "transcribe.convert_num_line_lowercase",
      "to": "transcribe.previous_word_exception"
    },
    {
      "from": "transcribe.convert_num_line_lowercase",
      "to": "transcribe.print_num_exception"
    },
    {
      "from": "transcribe.skip_speaker_line_with_timestamp",
      "to": "fileops.get_timestamp"
    },
    {
      "from": "transcribe.convert_num_lines",
      "to": "transcribe.convert_num_line_capitalization"
    },
    {
      "from": "transcribe.convert_num_lines",
      "to": "transcribe.convert_num_line_lowercase"
    },
    {
      "from": "transcribe.convert_num_lines",
      "to": "transcribe.skip_speaker_line_with_timestamp"
    },
    {
      "from": "transcribe.convert_numbers_in_content",
      "to": "transcribe.convert_num_lines"
    },
    {
      "from": "transcribe.convert_nums_to_words",
      "to": "fileops.read_file_flex"
    },
    {
      "from": "transcribe.convert_nums_to_words",
      "to": "fileops.verbose_print"
    },
    {
      "from": "transcribe.convert_nums_to_words",
      "to": "fileops.write_metadata_and_content"
    },
    {
      "from": "transcribe.convert_nums_to_words",
      "to": "transcribe.convert_numbers_in_content"
    },
    {
      "from": "transcribe.convert_nums_to_words",
      "to": "transcribe.convert_ordinals_in_content"
    },
    {
      "from": "transcribe.write_speaker_names_to_json",
      "to": "fileops.verbose_print"
    },
    {
      "from": "transcribe.find_unassigned_speakers",
      "to": "fileops.get_timestamp"
    },
    {
      "from": "transcribe.find_unassigned_speakers",
      "to": "fileops.verbose_print"
    },
    {
      "from": "transcribe.propagate_speaker_names_throughout_md",
      "to": "fileops.get_timestamp"
    },
    {
      "from": "transcribe.iterate_input_speaker_names",
      "to": "transcribe.find_unassigned_speakers"
    },
    {
      "from": "transcribe.iterate_input_speaker_names",
      "to": "transcribe.propagate_speaker_names_throughout_md"
    },
    {
      "from": "transcribe.assign_speaker_names",
      "to": "transcribe.iterate_input_speaker_names"
    },
    {
      "from": "transcribe.assign_speaker_names",
      "to": "transcribe.read_speaker_names_from_json"
    },
    {
      "from": "transcribe.assign_speaker_names",
      "to": "transcribe.write_speaker_names_to_json"
    },
    {
      "from": "transcribe.create_transcript_md_from_json",
      "to": "fileops.add_timestamp_links"
    },
    {
      "from": "transcribe.create_transcript_md_from_json",
      "to": "fileops.convert_seconds_to_timestamp"
    },
    {
      "from": "transcribe.create_transcript_md_from_json",
      "to": "fileops.create_initial_metadata"
    },
    {
      "from": "transcribe.create_transcript_md_from_json",
      "to": "fileops.set_metadata_field"
    },
    {
      "from": "transcribe.create_transcript_md_from_json",
      "to": "fileops.write_metadata_and_content"
    },
    {
      "from": "transcribe.create_transcript_md_from_json",
      "to": "transcribe.convert_nums_to_words"
    },
    {
      "from": "transcribe.create_transcript_md_from_json",
      "to": "transcribe.get_link_from_json"
    },
    {
      "from": "transcribe.create_transcript_md_from_json",
      "to": "transcribe.validate_transcript_json"
    },
    {
      "from": "transcribe.process_deepgram_transcription",
      "to": "transcribe.add_link_to_json"
    },
    {
      "from": "transcribe.process_deepgram_transcription",
      "to": "transcribe.assign_speaker_names"
    },
    {
      "from": "transcribe.process_deepgram_transcription",
      "to": "transcribe.create_transcript_md_from_json"
    },
    {
      "from": "transcribe.process_deepgram_transcription",
      "to": "transcribe.download_mp3_from_youtube"
    },
    {
      "from": "transcribe.process_deepgram_transcription",
      "to": "transcribe.transcribe_deepgram"
    },
    {
      "from": "transcribe.process_deepgram_transcription_from_audio_file",
      "to": "transcribe.add_link_to_json"
    },
    {
      "from": "transcribe.process_deepgram_transcription_from_audio_file",
      "to": "transcribe.assign_speaker_names"
    },
    {
      "from": "transcribe.process_deepgram_transcription_from_audio_file",
      "to": "transcribe.create_transcript_md_from_json"
    },
    {
      "from": "transcribe.process_deepgram_transcription_from_audio_file",
      "to": "transcribe.transcribe_deepgram"
    },
    {
      "from": "transcribe.process_multiple_videos",
      "to": "transcribe.create_youtube_md_from_file_link"
    },
    {
      "from": "transcribe.process_multiple_videos",
      "to": "transcribe.process_deepgram_transcription"
    },
    {
      "from": "llm.pretty_print_function",
      "to": "fileops.verbose_print"
    },
    {
      "from": "llm.pretty_print_function",
      "to": "llm.pretty_print_function_descriptions"
    },
    {
      "from": "llm.cost_llm_on_file",
      "to": "llm.count_tokens"
    },
    {
      "from": "llm.cost_llm_on_corpus",
      "to": "fileops.get_files_in_folder"
    },
    {
      "from": "llm.cost_llm_on_corpus",
      "to": "llm.cost_llm_on_file"
    },
    {
      "from": "llm.add_token_counts_to_headings",
      "to": "fileops.find_heading_text"
    },
    {
      "from": "llm.add_token_counts_to_headings",
      "to": "llm.count_tokens"
    },
    {
      "from": "llm.get_speaker_segments",
      "to": "fileops.get_heading"
    },
    {
      "from": "llm.count_segment_tokens",
      "to": "llm.count_tokens"
    },
    {
      "from": "llm.count_segment_tokens",
      "to": "llm.get_speaker_segments"
    },
    {
      "from": "llm.plot_segment_tokens",
      "to": "llm.count_segment_tokens"
    },
    {
      "from": "llm.plot_segment_tokens",
      "to": "llm.get_speaker_segments"
    },
    {
      "from": "llm.group_segments_select_speaker",
      "to": "fileops.get_timestamp"
    },
    {
      "from": "llm.split_file_select_speaker",
      "to": "fileops.read_metadata_and_content"
    },
    {
      "from": "llm.split_file_select_speaker",
      "to": "fileops.write_metadata_and_content"
    },
    {
      "from": "llm.split_file_select_speaker",
      "to": "llm.get_speaker_segments"
    },
    {
      "from": "llm.split_file_select_speaker",
      "to": "llm.group_segments_select_speaker"
    },
    {
      "from": "llm.split_file_every_speaker",
      "to": "fileops.read_metadata_and_content"
    },
    {
      "from": "llm.split_file_every_speaker",
      "to": "fileops.write_metadata_and_content"
    },
    {
      "from": "llm.split_file_every_speaker",
      "to": "llm.get_speaker_segments"
    },
    {
      "from": "llm.split_file_token_cap",
      "to": "fileops.read_metadata_and_content"
    },
    {
      "from": "llm.split_file_token_cap",
      "to": "fileops.write_metadata_and_content"
    },
    {
      "from": "llm.split_file_token_cap",
      "to": "llm.get_speaker_segments"
    },
    {
      "from": "llm.split_file_token_cap",
      "to": "llm.group_segments_token_cap"
    },
    {
      "from": "llm.test_openai_chat",
      "to": "llm.openai_chat_completion_request"
    },
    {
      "from": "llm.openai_function_call",
      "to": "fileops.verbose_print"
    },
    {
      "from": "llm.openai_function_call",
      "to": "llm.openai_chat_completion_request"
    },
    {
      "from": "llm.openai_function_call",
      "to": "llm.pretty_print_function"
    },
    {
      "from": "llm.llm_process_block",
      "to": "llm.openai_chat_completion_request"
    },
    {
      "from": "llm.llm_process_file_blocks",
      "to": "fileops.read_metadata_and_content"
    },
    {
      "from": "llm.llm_process_file_blocks",
      "to": "fileops.write_metadata_and_content"
    },
    {
      "from": "llm.llm_process_file_blocks",
      "to": "llm.llm_process_block"
    },
    {
      "from": "llm.scall_replace",
      "to": "llm.llm_process_file_blocks"
    },
    {
      "from": "llm.scall_append",
      "to": "llm.llm_process_file_blocks"
    },
    {
      "from": "llm.create_simple_llm_file",
      "to": "fileops.delete_file"
    },
    {
      "from": "llm.create_simple_llm_file",
      "to": "llm.scall_append"
    },
    {
      "from": "llm.create_simple_llm_file",
      "to": "llm.scall_replace"
    },
    {
      "from": "llm.create_copyedit_file",
      "to": "fileops.delete_file"
    },
    {
      "from": "llm.create_copyedit_file",
      "to": "llm.scall_replace"
    },
    {
      "from": "llm.mod_blocks_file_with_adjacent_words",
      "to": "fileops.read_metadata_and_content"
    },
    {
      "from": "llm.mod_blocks_file_with_adjacent_words",
      "to": "fileops.write_metadata_and_content"
    },
    {
      "from": "llm.scall_replace_adjacent_words",
      "to": "llm.llm_process_file_blocks"
    },
    {
      "from": "llm.scall_replace_adjacent_words",
      "to": "llm.mod_blocks_file_with_adjacent_words"
    },
    {
      "from": "llm.create_transitions_file",
      "to": "fileops.delete_file"
    },
    {
      "from": "llm.create_transitions_file",
      "to": "llm.scall_replace_adjacent_words"
    },
    {
      "from": "llm.fcall_qa_speaker",
      "to": "fileops.add_timestamp_links"
    },
    {
      "from": "llm.fcall_qa_speaker",
      "to": "fileops.read_metadata_and_content"
    },
    {
      "from": "llm.fcall_qa_speaker",
      "to": "fileops.set_last_updated"
    },
    {
      "from": "llm.fcall_qa_speaker",
      "to": "fileops.write_metadata_and_content"
    },
    {
      "from": "llm.fcall_qa_speaker",
      "to": "llm.openai_function_call"
    },
    {
      "from": "llm.fcall_qa_speaker",
      "to": "llm.pretty_print_function_descriptions"
    },
    {
      "from": "llm.fcall_qa_speaker",
      "to": "llm.tools_qa_speaker"
    },
    {
      "from": "llm.create_qa_file_select_speaker",
      "to": "fileops.delete_file"
    },
    {
      "from": "llm.create_qa_file_select_speaker",
      "to": "llm.fcall_qa_speaker"
    },
    {
      "from": "llm.create_qa_file_select_speaker",
      "to": "llm.split_file_select_speaker"
    },
    {
      "from": "llm.fcall_qa_incremental",
      "to": "llm.get_next_chunk"
    },
    {
      "from": "llm.fcall_qa_incremental",
      "to": "llm.openai_function_call"
    },
    {
      "from": "llm.fcall_qa_incremental",
      "to": "llm.tools_qa_incremental"
    },
    {
      "from": "llm.create_qa_file_from_transcript_incremental",
      "to": "fileops.get_heading"
    },
    {
      "from": "llm.create_qa_file_from_transcript_incremental",
      "to": "fileops.read_metadata_and_content"
    },
    {
      "from": "llm.create_qa_file_from_transcript_incremental",
      "to": "fileops.set_last_updated"
    },
    {
      "from": "llm.create_qa_file_from_transcript_incremental",
      "to": "fileops.set_metadata_field"
    },
    {
      "from": "llm.create_qa_file_from_transcript_incremental",
      "to": "fileops.write_metadata_and_content"
    },
    {
      "from": "llm.create_qa_file_from_transcript_incremental",
      "to": "llm.count_segment_tokens"
    },
    {
      "from": "llm.create_qa_file_from_transcript_incremental",
      "to": "llm.fcall_qa_incremental"
    },
    {
      "from": "llm.create_qa_file_from_transcript_incremental",
      "to": "structured.count_blocks"
    },
    {
      "from": "llm.evaluate_qa_extraction",
      "to": "fileops.read_metadata_and_content"
    },
    {
      "from": "llm.evaluate_qa_extraction",
      "to": "llm.openai_chat_completion_request"
    },
    {
      "from": "llm.evaluate_qa_extraction",
      "to": "llm.validate_qa_transcript_positions"
    },
    {
      "from": "llm.evaluate_qa_extraction",
      "to": "structured.count_blocks"
    },
    {
      "from": "llm.evaluate_qa_extraction",
      "to": "structured.get_all_fields_dict"
    },
    {
      "from": "llm.run_automated_evaluation",
      "to": "fileops.get_heading"
    },
    {
      "from": "llm.run_automated_evaluation",
      "to": "fileops.manage_file_overwrite"
    },
    {
      "from": "llm.run_automated_evaluation",
      "to": "llm.evaluate_qa_extraction"
    },
    {
      "from": "llm.run_automated_evaluation",
      "to": "llm.generate_evaluation_report"
    },
    {
      "from": "vectordb.generate_vectors_qa",
      "to": "fileops.get_files_in_folder"
    },
    {
      "from": "vectordb.generate_vectors_qa",
      "to": "fileops.get_timestamp"
    },
    {
      "from": "vectordb.generate_vectors_qa",
      "to": "structured.get_all_fields_dict"
    },
    {
      "from": "vectordb.generate_vectors_qa",
      "to": "structured.get_blocks_from_file"
    },
    {
      "from": "vectordb.generate_vectors_qa",
      "to": "vectordb.generate_embedding"
    },
    {
      "from": "vectordb.update_pinecone_index_list_md",
      "to": "fileops.get_current_datetime_humanfriendly"
    },
    {
      "from": "vectordb.setup_create_vectordb",
      "to": "fileops.get_current_datetime_filefriendly"
    },
    {
      "from": "vectordb.setup_create_vectordb",
      "to": "fileops.get_files_in_folder"
    },
    {
      "from": "vectordb.create_vectordb_vrag_langchain",
      "to": "fileops.apply_to_folder"
    },
    {
      "from": "fileops.apply_to_folder",
      "to": "fileops.create_new_file_from_heading",
      "type": "controller_to_worker",
      "applies_to_functions": [
        "vectordb.create_vectordb_vrag_langchain"
      ]
    },
    {
      "from": "vectordb.create_vectordb_vrag_langchain",
      "to": "fileops.move_files_with_suffix"
    },
    {
      "from": "vectordb.create_vectordb_vrag_langchain",
      "to": "vectordb.check_and_create_pinecone_index"
    },
    {
      "from": "vectordb.create_vectordb_vrag_langchain",
      "to": "vectordb.log_zip_vectordb"
    },
    {
      "from": "vectordb.create_vectordb_vrag_langchain",
      "to": "vectordb.save_splits_to_json"
    },
    {
      "from": "vectordb.create_vectordb_vrag_langchain",
      "to": "vectordb.setup_create_vectordb"
    },
    {
      "from": "vectordb.create_vectordb_vrag_langchain",
      "to": "vectordb.update_pinecone_index_list_md"
    },
    {
      "from": "vectordb.create_qrag_vectordb",
      "to": "vectordb.check_and_create_pinecone_index"
    },
    {
      "from": "vectordb.create_qrag_vectordb",
      "to": "vectordb.generate_vectors_qa"
    },
    {
      "from": "vectordb.create_qrag_vectordb",
      "to": "vectordb.log_zip_vectordb"
    },
    {
      "from": "vectordb.create_qrag_vectordb",
      "to": "vectordb.setup_create_vectordb"
    },
    {
      "from": "vectordb.create_qrag_vectordb",
      "to": "vectordb.update_pinecone_index_list_md"
    },
    {
      "from": "vectordb.create_qrag_vectordb",
      "to": "vectordb.upsert_vectors_pinecone"
    },
    {
      "from": "vectordb.create_qrag_vectordb",
      "to": "vectordb.vectors_to_json"
    },
    {
      "from": "rag.pinecone_retriever",
      "to": "vectordb.generate_embedding"
    },
    {
      "from": "rag.vrag_llm_call",
      "to": "llm.simple_openai_chat_completion_request"
    },
    {
      "from": "rag.vrag_llm_call",
      "to": "rag.pinecone_retriever"
    },
    {
      "from": "rag.parse_chunk_all",
      "to": "rag.safe_convert"
    },
    {
      "from": "rag.parse_chunk_qa_dd",
      "to": "rag.parse_chunk_all"
    },
    {
      "from": "rag.parse_chunk_qa_dd",
      "to": "rag.safe_float"
    },
    {
      "from": "rag.parse_chunk_qa_dd",
      "to": "rag.safe_int"
    },
    {
      "from": "rag.qrag_routing_call",
      "to": "rag.parse_chunk_qa_dd"
    },
    {
      "from": "rag.qrag_routing_call",
      "to": "rag.pinecone_retriever"
    },
    {
      "from": "rag.qrag_routing_call",
      "to": "rag.select_chunks_qrag_1or2"
    },
    {
      "from": "rag.qrag_llm_call",
      "to": "llm.simple_openai_chat_completion_request"
    },
    {
      "from": "rag.qrag_2step",
      "to": "rag.print_qrag_display_text"
    },
    {
      "from": "rag.qrag_2step",
      "to": "rag.qrag_llm_call"
    },
    {
      "from": "rag.qrag_2step",
      "to": "rag.qrag_routing_call"
    },
    {
      "from": "rag.safe_convert",
      "to": "rag.safe_convert"
    },
    {
      "from": "docwork.reformat_transcript_text",
      "to": "docwork.flush_current_dialogue"
    },
    {
      "from": "docwork.reformat_transcript_text",
      "to": "docwork.replace_colon_for_non_speaker"
    },
    {
      "from": "docwork.validate_transcript",
      "to": "fileops.get_heading"
    },
    {
      "from": "docwork.validate_transcript",
      "to": "fileops.verbose_print"
    },
    {
      "from": "docwork.extract_transcript_data",
      "to": "fileops.get_heading"
    },
    {
      "from": "docwork.extract_transcript_data",
      "to": "fileops.get_timestamp"
    },
    {
      "from": "docwork.create_speaker_triples",
      "to": "docwork.extract_transcript_data"
    },
    {
      "from": "docwork.create_speaker_matrix",
      "to": "fileops.apply_to_folder"
    },
    {
      "from": "fileops.apply_to_folder",
      "to": "docwork.create_speaker_triples",
      "type": "controller_to_worker",
      "applies_to_functions": [
        "docwork.create_speaker_matrix"
      ]
    },
    {
      "from": "docwork.create_speaker_matrix",
      "to": "fileops.create_csv_matrix_from_triples"
    },
    {
      "from": "docwork.extract_proper_names",
      "to": "fileops.verbose_print"
    },
    {
      "from": "docwork.create_proper_names_triples",
      "to": "docwork.extract_proper_names"
    },
    {
      "from": "docwork.print_proper_names",
      "to": "docwork.create_proper_names_triples"
    },
    {
      "from": "structured.get_blocks_from_file",
      "to": "fileops.get_heading"
    },
    {
      "from": "structured.count_blocks",
      "to": "fileops.get_heading"
    },
    {
      "from": "structured.extract_topic_counts_triples",
      "to": "structured.get_blocks_from_file"
    },
    {
      "from": "structured.extract_topic_counts_triples",
      "to": "structured.get_field_value"
    },
    {
      "from": "structured.create_topics_matrix",
      "to": "fileops.apply_to_folder"
    },
    {
      "from": "fileops.apply_to_folder",
      "to": "structured.extract_topic_counts_triples",
      "type": "controller_to_worker",
      "applies_to_functions": [
        "structured.create_topics_matrix"
      ]
    },
    {
      "from": "structured.create_topics_matrix",
      "to": "fileops.create_csv_matrix_from_triples"
    },
    {
      "from": "structured.change_topic_in_file",
      "to": "fileops.read_metadata_and_content"
    },
    {
      "from": "structured.change_topic_in_file",
      "to": "fileops.write_metadata_and_content"
    },
    {
      "from": "structured.change_topic_in_file",
      "to": "structured.get_blocks_from_file"
    },
    {
      "from": "structured.change_topic_in_folders",
      "to": "fileops.apply_to_folder"
    },
    {
      "from": "fileops.apply_to_folder",
      "to": "structured.process_file",
      "type": "controller_to_worker",
      "applies_to_functions": [
        "structured.change_topic_in_folders"
      ]
    },
    {
      "from": "structured.change_topic_in_folders",
      "to": "structured.change_topic_in_file"
    },
    {
      "from": "structured.review_singlet_topic_SONNET",
      "to": "structured.change_topic_in_file"
    },
    {
      "from": "structured.review_singlet_topic",
      "to": "structured.change_topic_in_file"
    },
    {
      "from": "structured.process_file",
      "to": "structured.change_topic_in_file"
    },
    {
      "from": "corpuses.is_valid_file_qa",
      "to": "corpuses.validate_qa_blocks"
    },
    {
      "from": "corpuses.is_valid_file_qa",
      "to": "structured.get_blocks_from_file"
    },
    {
      "from": "corpuses.validate_folders_qa",
      "to": "corpuses.is_valid_file_qa"
    },
    {
      "from": "corpuses.validate_folders_qa",
      "to": "fileops.get_files_in_folder"
    },
    {
      "from": "corpuses.validate_folders_qa",
      "to": "structured.get_blocks_from_file"
    },
    {
      "from": "corpuses.validate_corpus_deutsch",
      "to": "corpuses.validate_folders_qa"
    },
    {
      "from": "corpuses.clean_fda_townhall",
      "to": "corpuses.remove_lines_fda_townhall"
    },
    {
      "from": "corpuses.clean_fda_townhall",
      "to": "docwork.reformat_transcript_text"
    },
    {
      "from": "corpuses.clean_fda_townhall",
      "to": "fileops.get_heading"
    },
    {
      "from": "corpuses.clean_fda_townhall",
      "to": "fileops.set_heading"
    },
    {
      "from": "corpuses.run_clean_on_fda_townhalls",
      "to": "fileops.apply_to_folder"
    },
    {
      "from": "fileops.apply_to_folder",
      "to": "corpuses.clean_fda_townhall",
      "type": "controller_to_worker",
      "applies_to_functions": [
        "corpuses.run_clean_on_fda_townhalls"
      ]
    },
    {
      "from": "corpuses.run_clean_on_fda_townhalls",
      "to": "fileops.move_files_with_suffix"
    },
    {
      "from": "corpuses.validate_qa_blocks_townhall",
      "to": "corpuses.validate_qa_blocks"
    }
  ]
}